作为一个专业的python程序员，接下来你需要使用简洁及优雅的代码实现我的需求，所有函数必须有标准的英文注释，并且考虑错误处理及可测试性。理解请回复“OK”。


我希望实现一个类，它能够根据rss订阅列表，对其中的每一个rss feed，下载其中所有的文章内容，包括：
1. 文章标题
2. 发布时间
3. 文章链接
4. 文章内容
其中rss feed列表会通过读取json文件输入，json格式如下：
```
{
  "theme": "科技",
  "focus": "AI",
  "prompt": "",
  "feeds": {
    "青柠学术": "https://qnscholar.github.io//feed.xml",
    "MOOC中国": "https://www.cmooc.com/feed",
    "知乎日报": "https://feeds.feedburner.com/zhihu-daily",
    "知乎每日精选": "https://www.zhihu.com/rss"
  }
}
```
请将读取json文件、遍历feeds、对每个feed下载、对feed中其中一篇文章的链接下载各写在不同函数里。

对于上面的需求有不清楚之处清进一步询问，如果有更多的建议也请提出。




我希望将正文转换为markdown格式以减少复杂度。




在这个函数里面，调用另一个函数，用来将不同feed的文章分文件夹存储为<标题>.md文件。注意需要保证目录存在，且<标题>作为文件名有效，否则将其中非法字符进行清洗。
不要在最后一起写入，而是拉取一个写一个，这是为了防止中途出错。另外请考虑这么一种机制：成功拉取文章后记录一个“URL” - "文件"的持久性存储表，在每次拉取时，不要拉取已经拉取过的文章。





帮我写一个FeedsValidation的python脚本，提供本地API，WebAPI，命令行和界面四种界面。
本地API行为如下：
    传入一个URL，同步验证其是否能得到一个合法的RSS XML数据。
    传入一个Feeds字典，异步验证其中每一个URL能得到一个合法的RSS XML数据。
    维护一个 {Feed URL：状态} 字典，用户可以调用对应接口查询Feed的状态（未知，有效，无效，Busy等等）。
    提供另一个接口，清除Feed状态字典。
WebAPI则支持提交一个Feeds字典，同样更新到{Feed URL：状态} 字典，通过另一个wei api查询状态。
    同时给出调用该服务的测试代码。
命令行则接受一个url，同步验证其是否能得到一个合法的RSS XML数据，返回结果并退出。
界面则使用pyqt5，左侧输入单个链接或json，右侧则以列表显示每个feed链接的状态，feed状态的更新必须是异步的。
    用户可以勾选列表项，列表下方根据勾选项目动态生成json。程序提供“全选/全不选/仅选有效Feed/清除无效Feed”功能
如果不带命令行参数，默认启动界面，如果参数为url，则使用命令行。你来指定一个参数让可以启动web api服务。
将上面的功能实现在同一个py文件里，注意每个功能的独立性，比如pyqt5不存在不影响命令行的使用。

接受的JSON的格式如下：
"""
{
  "other keys just ignore": "........",
  "feeds": {
    "feed name 1": "feed url 1",
    "feed name 2": "feed url 2",
  }
}
"""

其中：
    列表要分列显示，列包括：Feed Name, Feed URL, Status
    动态生成的json项为  Feed Name: Feed URL，而非状态。





这个函数会遇到403的问题，我想，既然是验证RSS，那么正确的方法应该是利用标准的RSS请求来获取RSS资源，而非直接request爬取。
请帮我改造这段代码，在保持现有逻辑的前提下，使用标准的RSS请求来验证feed是否有效。






我使用feedparser库拉取并分析rss feed，但我发现该库反爬能力太弱，以致于对于很多feed根本拉取不到。
所以我希望通过无头浏览器拉取内容，再通过feedparser分析feed内容。
请帮我实现一个基于无头浏览器的强大网页内容获取程序，要求支持包括socks5在内的代理。





帮我实现一个parse_feed函数，参数是从rss feed返回的内容（content），你需要使用合适的库和工具对其进行解析。
该程序需要应对各种复杂和错误的情况，并且对每rss中的每一条数据，重新组织和返回关键字段的内容（请想一下rss的关键信息有哪些）。
注意不要出现语法错误和示实现的引用，不要简单的拼接代码，而是分析其功能和适用性，将其融合到自己的代码中。





我先给出requirements.txt中的内容，再给出所有python文件中import的内容，
帮我检查requirements.txt中是否有多余的依赖，或者缺失的依赖，最终输出完整且移除多余引用的requirements.txt内容。

requirements.txt :
"""
"""

Imports :
"""
"""







作为一个专业的python程序员，接下来你需要使用简洁及优雅的代码实现我的需求，所有函数必须有标准的英文注释，并且考虑错误处理及可测试性。

我希望通过url抓取一个网页，其中url来自RSS feed的XML文章描述。
帮我实现一个能应对大多数反爬情况的网页抓取模块，需要支持包括socks5在内的代理，但不需要考虑过于复杂的挑战的情况。
即尽可能地模拟浏览器浏览和渲染过程，需要考虑抓取的成功率，而不需要过多考虑抓取的效率。
需要考虑抓取函数可重入或抓取模块多实例化，以便多线程抓取。
请审视编写的代码，确定没有任何的引用未被实现。
在其它模块中使用了playwright，如果合适，你也可以考虑使用它。








我使用sync_playwright和requests实现不同模块来抓取网页，但它们的代理配置似乎不一样。

sync_playwright的格式是：

    proxy_config = {
        "server": "socks5://127.0.0.1:10808",
        "username": "",
        "password": ""
    }


而requests的格式是：


    proxies = {
        "http": "socks5://user:password@proxy_host:port",
        "https": "socks5://user:password@proxy_host:port"
    }


我希望它们能使用同样的格式，或者通过适配函数将格式转成一致。
注意要考虑到代理不需要认证，即username和password不存在的情况，一个函数要能同时处理有认证的情况和无认证的情况。
各个函数需要有标准英文注释，包括描述输入格式，输出格式。





针对 parse_proxy，to_playwright，to_requests，考虑各种情况，包括非法输入情况，为它们各自编写测试函数。
不需要使用任何测试框架，只需要使用assert进行断言即可。






在我的情报采集系统中，我想写一个prompt让AI给网络信息进行评分，分制为0 - 10分。
很显然，广告和完全无效的内容评分为0分。反之，诸如911和特朗普遇袭这类国际重大事件评为10分。
但对于中间具体的评分标准，我还没有思路。能不能帮助我制定关于情报价值评分标准的描述？如果有现成的成熟通用标准，那么最好。







帮我实现一个名为IntelligenceHub的类，它采用多线程实现以下功能：
1. 信息收集模块：提供WEB API，接受POST请求，参数是DICT，除UUID必须有之外，其余字段皆不限制，将收集信息放入待处理队列中。
    同时提供调用该WEB API的本地API（proxy）
2. 信息处理模块：从队列中获取待处理数据，通过POST请求调用WEB接口。该接口参数同样是DICT，除UUID外其余字段皆不限制。
   它将采用UUID作为消息的唯一标识，当消息异步处理结束后，该UUID将用来匹配之前的请求。
   如果一个数据在一定时间内未得到处理，将作为超时处理，将其放回待处理列表中，根据重试次数决定它的优先度。
3. 信息处理反馈模块
    提供WEB API，接受处理之后的消息，使用UUID匹配源消息。将源消息和处理后的消息一起放入队列中给后处理和归档模块。
    同时提供调用该WEB API的本地API（proxy）。
4. 后处理和归档模块：
    归档：将处理后的结果放入mongodb中，并建立指向它的向量数据库索引。
    后处理暂时未定，有可能采用插件的方式实现。
以上只是一个大概的框架，细节部分请加上TODO标记，由后续逐步实现。
一定一定要注意并发场景及效率，不要出现资源冲突的情况。



我实现了一个名为IntelligenceHub的类，它采用多线程实现以下功能：
1. 信息收集模块：提供WEB API，接受POST请求，参数是DICT，除UUID必须有之外，其余字段皆不限制，将收集信息放入待处理队列中。
    同时提供调用该WEB API的本地API（post_collected_intelligence）
2. 信息处理模块：从队列中获取待处理数据，通过POST请求调用WEB接口。该接口参数同样是DICT，除UUID外其余字段皆不限制。
   它将采用UUID作为消息的唯一标识，当消息异步处理结束后，该UUID将用来匹配之前的请求。
   如果一个数据在一定时间内未得到处理，将作为超时处理，将其放回待处理列表中，根据重试次数决定它的优先度。
3. 信息处理反馈模块
    提供WEB API，接受处理之后的消息，使用UUID匹配源消息。将源消息和处理后的消息一起放入队列中给后处理和归档模块。
    同时提供调用该WEB API的本地API（post_processed_intelligence）。
4. 后处理和归档模块：
    归档：将处理后的结果放入mongodb中，并建立指向它的向量数据库索引。
    后处理暂时未定，有可能采用插件的方式实现。

请一步步地分析该程序，帮我实现和改进你认为可以进一步实现或改进的地方。




请分析IntelligenceHub的功能和行为，并为IntelligenceHub编写包括Collector模拟, IntelligenceProcessor模拟在内的测试辅助模块。





基于它们编写测试用例，先测试整个处理流程，然后分条列出测试各种情况以及测试性能的测试用例简要描述。先不要输出测试代码。
先不要输出testcase的实现。






processor_thread是一个IO线程，因为等待超时所以会成为性能瓶颈。我希望按照CPU数除以2创建一个线程池，由该线程池来进行数据处理。
注意不要频繁创建和退出线程，而是通过信号里让线程等待数据，并且不影响检查退出标记。相关代码如下：








理解下面一段代码，增加一个类，功能如下：
    提供一个接口，参数与load_json_config()一样，名字请根据功能自行推测。
    调用该函数在读取json配置后，会创建一个线程以polling_interval周期循环执行processor.process_all_feeds()。
    要注意使用线程退出标志让线程有优雅退出的机会，以文件名为索引可以找到并管理线程。
    如果发现参数中的文件名已存在线程列表中，如果指定force，则关闭旧线程开启新线程，否则返回错误并忽略。
    该类同时还要提供统计信息获取接口，并注意多线程资源访问问题。







写一个函数，接受两个参数：key属性表，待检查的dict。
key属性表类似下面的例子：

COLLECTOR_DATA_FIELDS = {
    'UUID': 'M',
    'source': 'O',
}

基中M代表MUST，O代表OPTIONAL。函数返回 (MUST字段的缺失数，OPTIONAL字段的存在数，不在属性表中的字段数)






python有没有可能实现这样一种机制：
当我向特定文件夹放入一个python文件时，这个新加入文件会被服务识别并从指定的函数启动。
当我从这个文件夹中移除一个文件时，服务会在该文件对应的任务执行完毕后移除该任务（移除文件对程序运行本身没有影响）。
请仔细分析这个需求，分析其中的问题及实现可能性，给出上面提到的“服务”程序的代码。







python有没有这样的机制：开辟一个有P个线程的线程池，执行T个任务（T可能大于P）。
里面的任务不会发生抢占







下面这段代码，由于一个sql连接无法在线程间共享，因此我希望将整个save_content做成异步的。
即在该单例初始化时创建一个线程，调用save_content时仅把数据放入队列中，并通知线程做后续操作。
该类需提供优雅退出线程的方法，并且注意线程安全问题。



使用crawl4ai实现以下接口，要求能对抗大部分常用的反爬手段，并且提供分析抓取失败的诊断手段。


class ProxyConfig(TypedDict):
    http: str
    https: str


class ScraperResult(TypedDict):
    content: str
    errors: List[str]


def fetch_content(
    url: str,
    timeout_ms: int,
    proxy: Optional[ProxyConfig] = None,
    **kwargs
) -> ScraperResult:




写一段python程序，使用openai的接口调用硅基流动的API接口（是不是理论上可以支持所有openai like接口？）。
token可以在构造时从外界传入，如果没有，则去环境变量中获取。
可以选择采用同步或异步的方式获取结果。
所有函数都需要标准注释，且关键地方也需要注释说明意图和数据格式，所有注释必须为英文。













基于我们的对话上下文，请分析并理解我的需求，基于上面的AI接口，编写一个函数实现下面描述的功能：

输入：主prompt，结构化的数据（json，其中主要内容是content字段，其它比如title字段等等为可选），对话上下文（可选）
输出：按prompt输出的json格式。
函数功能：将输入的prompt和结构化数据传递给AI，等待输出。输出应当为json格式，将其转为dict输出。

主prompt示例如下：









程序中已通过以下代码初始化好了mongodb client：

"""
def _setup_mongo_db(self) -> bool:
    """初始化MongoDB连接并创建必要索引"""
    try:
        # 连接配置
        self.mongo_client = pymongo.MongoClient(
            self.mongo_db_uri,
            maxPoolSize=100,
            serverSelectionTimeoutMS=5000
        )

        # 验证连接
        self.mongo_client.admin.command('ping')
        self.db = self.mongo_client["intelligence_db"]
        self.archive_col = self.db["processed_data"]

        # 定义索引规范（名称、字段、选项）
        index_specs = [
            {
                "name": "created_at_idx",
                "keys": [("metadata.created_at", pymongo.ASCENDING)],
                "description": "基于创建时间的时间范围查询加速"
            },
            {
                "name": "vector_dim_idx",
                "keys": [("vector.dim", pymongo.ASCENDING)],
                "options": {
                    "partialFilterExpression": {"vector": {"$exists": True}},
                    "background": True  # 后台创建避免阻塞
                },
                "description": "向量维度查询优化（仅当存在vector字段时建立）"
            },
            {
                "name": "content_text_idx",
                "keys": [("raw_data.value", "text")],
                "options": {
                    "weights": {"raw_data.value": 5},
                    "default_language": "english",
                    "language_override": "language"
                },
                "description": "全文检索索引（字段权重优化）"
            }
        ]

        # 检查并创建索引
        existing_indexes = {idx["name"]: idx for idx in self.archive_col.list_indexes()}

        for spec in index_specs:
            index_name = spec["name"]
            index_model = pymongo.IndexModel(spec["keys"], **spec.get("options", {}))

            # 索引存在性检查
            if index_name in existing_indexes:
                existing = existing_indexes[index_name]
                # 检查索引定义是否一致
                if (existing["key"] == index_model.document['key'] and
                        existing.get("partialFilterExpression") == spec.get("options", {}).get(
                            "partialFilterExpression")):
                    logger.info(f"索引 {index_name} 已存在，跳过创建")
                    continue

                # 删除不一致的旧索引
                logger.warning(f"检测到不一致索引 {index_name}，重新创建...")
                self.archive_col.drop_index(index_name)

            # 创建新索引
            logger.info(f"创建索引 {index_name}：{spec['description']}")
            self.archive_col.create_indexes([index_model])

        return True
    except pymongo.errors.OperationFailure as e:
        logger.critical(f"MongoDB索引操作失败: {str(e)}")
        return False
    except Exception as e:
        logger.critical(f"MongoDB连接失败: {str(e)}")
        return False
"""


请写一个函数将dict写入该数据库。dict示例如下（省略部分数据）：

"""
C:\D\Code\git\IntelligenceIntegrationSystem\.venv\Scripts\python.exe -X pycache_prefix=C:\Users\yq1061170\AppData\Local\JetBrains\PyCharmCE2024.2\cpython-cache "C:/Program Files/JetBrains/PyCharm Community Edition 2024.2.3/plugins/python-ce/helpers/pydev/pydevd.py" --multiprocess --qt-support=auto --client 127.0.0.1 --port 50471 --file C:\D\Code\git\IntelligenceIntegrationSystem\Tools\IntelligenceAnalyzerProxy.py
Connected to pydev debugger (build 242.23339.19)
AI response spends 94.8275637626648 s
{
  "UUID": "55eebb04-1400-4b33-9800-7fdb43ad4689",
  "TIME": "2024-07-19",
  "LOCATION": "",
  "PEOPLE": [],
  "ORGANIZATION": [
    "Netflix",
    "Paramount",
    "Disney",
    "Amazon",
    "Corus",
    "Global TV",
    "",
    "Motion Picture Association-Canada"
  ],
  "EVENT_BRIEF": "",
  "EVENT_TEXT": ",
  "RATE": {
    "战略相关性": 6,
    "国际关联度": 4,
    "金融影响力": 6,
    "政策关联度": 8,
    "科技前瞻性": 2,
    "投资价值": 6,
    "内容准确率": 8
  },
  "IMPACT": "",
  "TIPS": "流媒体与传统媒体就加拿大内容资金义务产生政策博弈"
}

Process finished with exit code 0

"""




作为一个资深专业的python程序员，帮我实现一个mongodb存储类，要求如下：
1. 存储通用的dict，而不要去管dict中的键值。
2. 提供基本的查询接口。
3. 良好的可配置性，如服务地址，数据库名，登录认证等等，参数配置需配上详细说明。
4. 多线程访问安全，保证资源不泄露。
5. 适度通用的优化，以增加访问速度，优化需要配上详细说明。
6.注释必须标准且为英文，所有文字都需要是标准专业的英文。




使用并改造以下的类，实现以下功能：
1. 增加一个update的方法，指定一个或多个字段以及值（dict），并指定需要更新或增加的字段（dict），更新对应记录。
2. 增加一个查询接口，指定，指定一个或多个字段以及值（dict），返回记录的dict列表（转成dict，而非原始的查询记录）











使用python，基于faiss实现一个向量数据库类，提供以下功能：

1. 能指定存储路径或数据库名
2. 指定一个id(str)和一段文本，为文本生成相应的向量并与id关联存储
3. 支持查询功能，提供一段文本和topN，返回匹配的id list。

要求使用合适的尽可能简单和轻量的实现，不要使用过于重型和收费的库。









帮我检查这段代码，一步步分析代码的功能和作者的用意，从以下角度思考：
1. 代码是否有问题或漏洞
2. 代码是否健壮
3. 代码是否有可以改进的地方
4. 考虑多线程的问题
综合以上分析，最终给出你的改进建议。





现在（2025年）python接入微信聊天（非小程序）的方案有哪些？分别有什么限制和特点？


有什么开源或开放的IM软件，满足以下需求：
1. 包含Android客户端和PC客户端
2. 支持python接入
3. 国内易于访问






使用python写一个RSS的发布程序，一方面接收远程或本地的调用请求，根据提供的信息建立RSS信息库，另一方面将这个信息按指定端口提供标准的RSS Feed。
注意要考虑到信息的滚动更新，并且需要考虑到在被调用和进行推送之前，启动时的初始Feed列表如何建立。究竟是使用推送的方式，还是使用类似代理的方式，由你统筹考虑后决定。
注意核心代码中不要涉及任何有关webservice的代码，核心类仅提供API供webservice调用。webservice在测试代码中实现，并独立于RSS核心类。
注意所有文本，包括注释需要使用英文。


MongoDB里存储了一系列dict数据，key的描述如下：

"""
    UUID: str
    TIME: str | None = None
    LOCATION: list | None = None
    PEOPLE: list | None = None
    ORGANIZATION: list | None = None
    EVENT_BRIEF: str | None = None
    EVENT_TEXT: str | None = None
    RATE: dict | None = {}
    IMPACT: str | None = None
    TIPS: str | None = None
"""

基于以上信息，请理解并根据以下函数的参数，构造查询

"""
    def query_intelligence(self,
                           *,
                           db: str = 'cache',
                           period:      Optional[Tuple[datetime.date, datetime.date]] = None,
                           locations:   Optional[str, List[str]] = None,
                           peoples:     Optional[str, List[str]] = None,
                           organizations: Optional[str, List[str]] = None,
                           keywords: Optional[str] = None
                           ):
        pass
"""

另一个涉及查询的参考代码如下：

"""
    def _load_unarchived_data(self):
        """Load unarchived data into a queue."""
        if not self.mongo_db_cache:
            return

        try:
            # 1. Build query
            query = {APPENDIX_ARCHIVED_FLAG: {"$exists": False}}

            # 2. Stream processing
            cursor = self.mongo_db_cache.collection.find(query)
            for doc in cursor:
                # Convert ObjectId to string
                doc['_id'] = str(doc['_id'])

                # 3. Handle queue with timeout
                try:
                    self.original_queue.put(doc, block=True, timeout=5)
                except queue.Full:
                    logger.error("Queue full, failed to add document")
                    break

            logger.info(f'Previous unprocessed data loaded, item count: {self.original_queue.qsize()}')

        except pymongo.errors.PyMongoError as e:
            logger.error(f"Database operation failed: {str(e)}")
"""





















使用python写一个函数 default_article_render() ，传入下面这样的一个dict，内容描述如下。要求将其内容组织为易于阅读且美观的HTML，便于用户浏览。其中使用的css最好是网上通用的在线格式。

{
  "UUID": "将输入的UUID原封不动地放在这里",
  "INFORMANT": "如果输入包含URL，则将输入的URL放在这里，否则查找来源信息，如果没有相关信息，则为空"
  "TIME": "YYYY-MM-DD格式，无则null",
  "LOCATION": ["文章主体中涉及的 国家/省/市/具体地址，无则为空列表"],
  "PEOPLE": ["文章主体中涉及的 姓名列表, 无则为空列表"],
  "ORGANIZATION": ["文章主体中涉及的 国家/公司/宗教/机构/组织名称等, 无则为空列表"],
  "EVENT_TITLE": "20字内描述核心内容的标题",
  "EVENT_BRIEF": "50字内描述核心事实",
  "EVENT_TEXT": "去除广告及主题无关信息后，干净简洁的文章内容",
  "RATE": {
    "战略相关性": "0-10分偶数",
    "国际关联度": "0-10分偶数",
    "金融影响力": "0-10分偶数",
    "政策关联度": "0-10分偶数",
    "科技前瞻性": "0-10分偶数",
    "投资价值": "0-10分偶数",
    "内容准确率": "0-10分偶数",
  },
  "IMPACT": "该事件可能的影响及推论，主要是对资产的影响，如无明显影响，则为空",
  "TIPS": "你给用户的备注信息，不要超过50个字"
}






实现以下函数：

def dump_text(self) -> str:
    pass

这个函数用以将 self.config_data 这个dict dump为易读的文本。
关键点在于：self.config_data会嵌套其它dict，对于嵌套的dict，输出的key以"."分隔。例如：

```
{
  "intelligence_hub": {

    "ai_service": {
      "url": "http://localhost:11434",
      "token": "SleepySoft",
      "model": "qwen3:14b"
    }
}
```

dump示例为：

intelligence_hub.ai_service.url: "http://localhost:11434"
intelligence_hub.ai_service.token: "SleepySoft"
intelligence_hub.ai_service.model: "qwen3:14b"

你可以自由调整对齐和分隔符等使得输出更加好看易读。







我有以下prompt，但其在规模较小的模型上表现不稳定，请分析原因和优化以下的prompt，使其表述更精辟和精确，同时在较小的模型上有同样好的表现。
要求检查原prompt中的错误，细化"RATE"中的评分标准，给出更典型的输出示例，以准确指导模型准确输出内容。


DEFAULT_ANALYSIS_PROMPT = """# 角色设定
你是一个专业情报分析师，需对输入文本进行结构化解析与价值评分

注意：对于文学性，介绍性等没有情报价值，或情报价值较低的文章，不要犹豫，直接返回以下json字符串：

{
  "UUID": "将输入的UUID原封不动地放在这里"
}

排除没有情报价值的文章后，按下面的要对文章进行分析。

# 处理流程
1. 按字段顺序提取五要素
2. 生成信息浓缩的标题
3. 按评分标准进行维度评分
4. 输出严格符合JSON格式
5. 无论原文章是什么语言，处理后的结果必须是中文

# 输出要求
{
  "UUID": "将输入的UUID原封不动地放在这里",
  "INFORMANT": "如果输入包含URL，则将输入的URL放在这里，否则查找来源信息，如果没有相关信息，则为空"
  "TIME": "YYYY-MM-DD格式，无则null",
  "LOCATION": ["文章主体中涉及的 国家/省/市/具体地址，无则为空列表"],
  "PEOPLE": ["文章主体中涉及的 姓名列表, 无则为空列表"],
  "ORGANIZATION": ["文章主体中涉及的 国家/公司/宗教/机构/组织名称等, 无则为空列表"],
  "EVENT_TITLE": "20字内描述核心内容的标题",
  "EVENT_BRIEF": "50字内描述核心事实",
  "EVENT_TEXT": "去除广告及主题无关信息后，干净简洁的文章内容。对于翻译的内容，注意语言流畅及本地化，禁止翻译腔",
  "RATE": {
    "战略相关性": "0-10分偶数",
    "国际关联度": "0-10分偶数",
    "金融影响力": "0-10分偶数",
    "政策关联度": "0-10分偶数",
    "科技前瞻性": "0-10分偶数",
    "投资价值": "0-10分偶数",
    "内容准确率": "0-10分偶数",
  },
  "IMPACT": "该事件可能的影响及推论，主要是对资产的影响，如无明显影响，则为空",
  "TIPS": "你给用户的备注信息，不要超过50个字"
}

# 评分细则（仅取整数分）
## 战略相关性（25%权重）
10分：改变国际力量格局（如战争、科技封锁）
8分：引发区域经济重构（如自贸区建立）
6分：国家战略级政策发布（国家级别）
4分：行业标准变更（行业级别）
2分：地方性政策

## 国际关联度（20%权重）
10分：涉及3+主要经济体
8分：双边协议影响供应链
6分：单边政策全球波及
4分：跨国企业重大调整

## 金融影响力（15%权重）
10分：引发全球市场剧烈波动
8分：改变大宗商品定价
6分：资本流动规则变更
4分：融资环境重大变化

## 政策关联度（15%权重）
10分：国家级产业政策突破
8分：监管框架重构
6分：税收/补贴调整
4分：地方试点推广

## 科技前瞻性（10%权重）
10分：颠覆性技术突破
8分：卡脖子技术进展
6分：专利格局变化
4分：科研投入转向

## 投资价值（10%权重）
10分：创造新产业赛道
8分：改变估值逻辑
6分：影响资产配置
4分：催生金融产品

## 内容准确率（5%权重）
10分：多方权威验证
8分：含原始数据
6分：逻辑自洽推测
4分：部分数据存疑

# 输出示例
输入："
- UUID: 218bd628-33c4-401a-b081-313f9da653a9
- title: 新华社消息
- authors: ['胡锡桧']
- pub_time: published
- informant: https://www.cbc.ca/news/science

## 正文内容
2024年7月19日，中美在APEC峰会达成人工智能芯片出口管制协议，涉及英伟达、AMD等企业
"

输出：
{
  "UUID": "218bd628-33c4-401a-b081-313f9da653a9",
  "INFORMANT": "https://www.cbc.ca/news/science",
  "TIME": "2024-07-19",
  "LOCATION": "APEC峰会",
  "PEOPLE": [],
  "ORGANIZATION": ["中国","美国","英伟达","AMD"],
  "EVENT_TITLE": "中美达成AI芯片出口管制协议",
  "EVENT_BRIEF": "中美在APEC峰会上达成AI芯片出口管制协议",
  "EVENT_TEXT": "2024年7月19日，中美在APEC峰会达成人工智能芯片出口管制协议，涉及英伟达、AMD等企业",
  "RATE": {
    "战略相关性":6,
    "国际关联度":8,
    "金融影响力":8,
    "政策关联度":8,
    "科技前瞻性":6,
    "投资价值":8,
    "内容准确率":10
  }
  "IMPACT": "该事件有可能影响AI相关芯片的价格",
  "TIPS": ""
}

# 异常处理
- 存在矛盾信息时取最低维度分
- 广告内容所有维度均计0分
- 无法判断的字段用null表示
"""







我先给出一份requirements.txt中的内容，里面会列出依赖的库，但并没有具体的版本信息。
接下来我会给出pip freeze的输出，毫无疑问，其中会包含详细的版本信息，但也会包含不需要的库，或间接依赖的库。
我希望：
1. 参考 requirements.txt ，并且根据 pip freeze的输出，给出一份精简但包含正确版本信息的 requirements.txt 信息。
2. 对于输出的 requirements.txt 中的每一项，加上对齐的注释，说明其主要用途，以及有需要的，任何附加说明。
3. 输出的 requirements.txt 最好能注意分组和美观性。

```requirements.txt
lxml
chardet
fastapi
uvicorn
cryptography
bs4~=0.0.1
Flask>=2.0.3
PyQt5>=5.15.6
tenacity>=8.2.3
feedparser==6.0.10
markdownify~=1.1.0
brotli
requests[socks]>=2.26.0
pymongo
pytest
werkzeug
html2text
watchdog
pytest-asyncio
tldextract

urllib3>=1.26.9,<1.27
beautifulsoup4>=4.12
playwright>=1.49.0					            # Then run: playwright install chromium
crawl4ai==0.6.3

numpy
pandas

# transformers==4.36.0
# sentence-transformers
# hnswlib

faiss-cpu           # Windows
# faiss-gpu           # Linux/Mac with NVIDIA card
huggingface_hub[hf_xet]


typing_extensions
tortoise-orm

PyRSS2Gen
```


```pip freeze
aiofiles==24.1.0
aiohappyeyeballs==2.6.1
aiohttp==3.12.2
aiosignal==1.3.2
aiosqlite==0.21.0
annotated-types==0.7.0
anyio==4.9.0
async-timeout==5.0.1
attrs==25.3.0
beautifulsoup4==4.13.4
blinker==1.9.0
Brotli==1.1.0
bs4==0.0.2
certifi==2025.4.26
cffi==1.17.1
chardet==5.2.0
charset-normalizer==3.4.2
click==8.2.1
colorama==0.4.6
Crawl4AI==0.6.3
cryptography==45.0.3
cssselect==1.3.0
distro==1.9.0
dnspython==2.7.0
exceptiongroup==1.3.0
faiss-cpu==1.11.0
fake-http-header==0.3.5
fake-useragent==2.2.0
fastapi==0.115.12
feedparser==6.0.10
filelock==3.18.0
Flask==3.1.1
frozenlist==1.6.0
fsspec==2025.5.1
greenlet==3.2.2
h11==0.16.0
hnswlib==0.8.0
html2text==2025.4.15
httpcore==1.0.9
httpx==0.28.1
httpx-aiohttp==0.1.4
huggingface-hub==0.32.2
humanize==4.12.3
idna==3.10
importlib_metadata==8.7.0
iniconfig==2.1.0
iso8601==2.1.0
itsdangerous==2.2.0
Jinja2==3.1.6
jiter==0.10.0
joblib==1.5.1
jsonschema==4.24.0
jsonschema-specifications==2025.4.1
litellm==1.71.1
lxml==5.4.0
markdown-it-py==3.0.0
markdownify==1.1.0
MarkupSafe==3.0.2
mdurl==0.1.2
mpmath==1.3.0
multidict==6.4.4
networkx==3.4.2
nltk==3.9.1
numpy==2.2.6
openai==1.82.0
packaging==25.0
pandas==2.2.3
pillow==10.4.0
playwright==1.52.0
pluggy==1.6.0
propcache==0.3.1
psutil==7.0.0
pycparser==2.22
pydantic==2.11.5
pydantic_core==2.33.2
pyee==13.0.0
Pygments==2.19.1
pymongo==4.13.0
pyOpenSSL==25.1.0
pyperclip==1.9.0
pypika-tortoise==0.6.1
PyQt5==5.15.11
PyQt5-Qt5==5.15.2
PyQt5_sip==12.17.0
PyRSS2Gen==1.1
PySocks==1.7.1
pytest==8.3.5
pytest-asyncio==1.0.0
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
pytz==2025.2
PyYAML==6.0.2
rank-bm25==0.2.2
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
requests-file==2.1.0
rich==14.0.0
rpds-py==0.25.1
safetensors==0.5.3
scikit-learn==1.6.1
scipy==1.15.3
sentence-transformers==3.0.1
sgmllib3k==1.0.0
six==1.17.0
sniffio==1.3.1
snowballstemmer==2.2.0
soupsieve==2.7
starlette==0.46.2
sympy==1.14.0
tenacity==9.1.2
tf-playwright-stealth==1.1.2
threadpoolctl==3.6.0
tiktoken==0.9.0
tldextract==5.3.0
tokenizers==0.15.2
tomli==2.2.1
torch==2.7.0
tortoise-orm==0.25.0
tqdm==4.67.1
transformers==4.36.0
typing-inspection==0.4.1
typing_extensions==4.13.2
tzdata==2025.2
urllib3==1.26.20
uvicorn==0.34.2
watchdog==6.0.0
Werkzeug==3.1.3
xxhash==3.5.0
yarl==1.20.0
zipp==3.22.0
```





分析并理解以下代码，做以下重构：

所有注释和文本皆为英文
query_intelligence增加条数限制参数
除非有重大问题，否则代码不要大改，否则难以比对你所更新的地方。







分析以下代码，增加几个函数，注意增加的函数注释及文本需要使用英文，下面提供的函数名仅供参考，请根据语义取更合适的名字：

get_intelligence_count_and_base: 获取当前所有的条目数据，同时返回首条索引，作为base
get_intelligence_by_base_offset: 将base，offset，count传入，获取以base为基准，offset后的count条数据（用做数据浏览，指定base以免新增条目引起跳动）







使用python写一个函数 default_article_list_render() ，传入下面这样的dict列表，dict内容描述如下。要求：
1. 将列表内容组织为易于阅读且美观的HTML，便于用户浏览。其中使用的css最好是网上通用的在线格式。
2. 标题显示超链接，格式为：'/intelligence/<string:intelligence_uuid>'。暂时使用相对路径url，点击在新页面打开该url。
3. 下方显示翻页按钮，当前页面的参数为“/intelligences?offset=100,count=20”，则上一页的url为“/intelligences?offset=80,count=20”，下一页为“/intelligences?offset=120,count=20”，以此类推。尽可能使用简单的方法实现这一功能。
4. 返回HTML，可以带有简单的样式表和脚本，不得过于复杂。
5. 注意该功能不使用前后端，如描述所示，所有页面由服务器生成。

{
  "UUID": "将输入的UUID原封不动地放在这里",
  "INFORMANT": "如果输入包含URL，则将输入的URL放在这里，否则查找来源信息，如果没有相关信息，则为空"
  "TIME": "YYYY-MM-DD格式，无则null",,
  "EVENT_TITLE": "20字内描述核心内容的标题",
  "EVENT_BRIEF": "50字内描述核心事实",
}







请分析以下函数，分析其各参数的类型（字符或列表，时间等）及关系（与/或，同时存在或互斥），写一个网页表单对其进行查询，并完善路由函数。

注意：
1. 所有文本需要使用英文，并考虑查询结果过多需要分页的问题。
2. 当访问/intelligences/query时，展示该网页。上半部分为表单（包括页码选择），下半部分为结果列表。所有内容由服务端生成，不要使用动态网页。

```

    def query_intelligence(
            self,
            *,
            period: Optional[Tuple[datetime.datetime, datetime.datetime]] = None,
            locations: Optional[Union[str, List[str]]] = None,
            peoples: Optional[Union[str, List[str]]] = None,
            organizations: Optional[Union[str, List[str]]] = None,
            keywords: Optional[str] = None,
            skip: Optional[int] = None,
            limit: Optional[int] = None
    ) -> List[dict]:
        """Execute intelligence query

        Args:
            period: UTC time range (start, end)
            locations: Location ID(s) (str or str list)
            peoples: Person ID(s) (str or str list)
            organizations: Organization ID(s) (str or str list)
            keywords: Full-text keywords
            skip: Number of documents to skip
            limit: Maximum number of results to return

        Returns:
            List of matching intelligence documents
        """
        # Get specified database collection
        collection = self.__mongo_db.collection

        try:
            # Build MongoDB query
            query = self.build_intelligence_query(
                period=period,
                locations=locations,
                peoples=peoples,
                organizations=organizations,
                keywords=keywords
            )

            # Execute query and return results with limit
            return self.execute_query(collection, query, skip=skip, limit=limit)

        except pymongo.errors.PyMongoError as e:
            logger.error(f"Intelligence query failed: {str(e)}")
            return []
        except Exception as e:
            logger.error(f"Intelligence query error: {str(e)}", stack_info=True)
            return []


    def build_intelligence_query(
            self,
            period: Optional[Tuple[datetime.datetime, datetime.datetime]] = None,
            locations: Optional[Union[str, List[str]]] = None,
            peoples: Optional[Union[str, List[str]]] = None,
            organizations: Optional[Union[str, List[str]]] = None,
            keywords: Optional[str] = None
    ) -> dict:
        query_conditions = []

        if period:
            query_conditions.append(self.build_time_condition(*period))

        if locations:
            query_conditions.append(self.build_list_condition("LOCATION", locations))

        if peoples:
            query_conditions.append(self.build_list_condition("PEOPLE", peoples))

        if organizations:
            query_conditions.append(self.build_list_condition("ORGANIZATION", organizations))

        if keywords:
            query_conditions.append(self.build_keyword_or_condition(keywords))

        return {"$and": query_conditions} if query_conditions else {}
```


```
@self.app.route('/intelligences/query', methods=['GET', 'POST'])
def intelligences_query_api():
    # TODO: self.intelligence_hub.query_intelligence(...)
    pass
```





帮我写一个文章管理的功能，当有请求到来时，检查对应的markdown文件，并生成对应的html文件到template目录下。

为了避免反复生成，在转换成html时会记录markdown文件的hash，如果hash相同且对应html文件存在，则不会重新生成。

为了避免和手写的template混淆，生成的html位于generated子目录下，生成文件之前要确保该目录存在。










mongodb的记录中有RATE字段，内容为{ "评分维度": "评分" }。评分维度非固定，评分统一为0 - 10分。
我想筛选出只要一个评分维度大于阈值N分的记录，这个查询语句应该怎么写？









有没有python的log，分类浏览，统计，网页仪表盘的整合解决方案。






分析我的意图并帮我完善代码，注意注释文本都需要使用英文：

class CrawlStatistics:
    # TODO: Singleton with multi-thread protecting

    def __init__(self):
        self._counter_log_lock = threading.Lock()
        self._counter_log_record = {}

        self._sub_item_log_lock = threading.Lock()
        self._sub_item_log_record = {}

    def counter_log(self, leveled_names: List[str], counter_item_name: str, log_text: str):
        with self._counter_log_lock:
            # TODO: Create leveled item and +1
            pass

    def get_classified_counter(self, leveled_names: List[str]) -> dict:
        # TODO: Return all { counter_item_name: count }
        pass

    def sub_item_log(self, leveled_names: List[str], sub_item, status: str):
        with self._sub_item_log_lock:
            # TODO: Create leveled item and set status
            pass

    def get_sub_item_statistics(self, leveled_names: List[str]):
        # TODO: Return { status: subitem }
        pass









请理解为该类编写测试用例，请考虑各种情况，且准备足够的数据。注意：不要使用随机数据进行测试











python的requests和playwright使用的proxy参数格式是不同的，一个是
        {
            "http": "socks5://user:password@proxy_host:port",
            "https": "socks5://user:password@proxy_host:port"
        }
另一个是
            {
                "server": "proto://host:port",
                "username": "",  # Explicit empty string if no auth
                "password": ""   # Explicit empty string if no auth
            }

我希望对于一个给定的代理，无论是什么格式，先转换成一个中间格式，再用两个函数分别转成requests及playwright格式。这个函数需要考虑到常见的代理格式及异常和错误格式处理，可以打印错误信息，但不能抛出异常。所有注释及文本需要使用英文。



