作为一个专业的python程序员，接下来你需要使用简洁及优雅的代码实现我的需求，所有函数必须有标准的英文注释，并且考虑错误处理及可测试性。理解请回复“OK”。


我希望实现一个类，它能够根据rss订阅列表，对其中的每一个rss feed，下载其中所有的文章内容，包括：
1. 文章标题
2. 发布时间
3. 文章链接
4. 文章内容
其中rss feed列表会通过读取json文件输入，json格式如下：
```
{
  "theme": "科技",
  "focus": "AI",
  "prompt": "",
  "feeds": {
    "青柠学术": "https://qnscholar.github.io//feed.xml",
    "MOOC中国": "https://www.cmooc.com/feed",
    "知乎日报": "https://feeds.feedburner.com/zhihu-daily",
    "知乎每日精选": "https://www.zhihu.com/rss"
  }
}
```
请将读取json文件、遍历feeds、对每个feed下载、对feed中其中一篇文章的链接下载各写在不同函数里。

对于上面的需求有不清楚之处清进一步询问，如果有更多的建议也请提出。




我希望将正文转换为markdown格式以减少复杂度。




在这个函数里面，调用另一个函数，用来将不同feed的文章分文件夹存储为<标题>.md文件。注意需要保证目录存在，且<标题>作为文件名有效，否则将其中非法字符进行清洗。
不要在最后一起写入，而是拉取一个写一个，这是为了防止中途出错。另外请考虑这么一种机制：成功拉取文章后记录一个“URL” - "文件"的持久性存储表，在每次拉取时，不要拉取已经拉取过的文章。





帮我写一个FeedsValidation的python脚本，提供本地API，WebAPI，命令行和界面四种界面。
本地API行为如下：
    传入一个URL，同步验证其是否能得到一个合法的RSS XML数据。
    传入一个Feeds字典，异步验证其中每一个URL能得到一个合法的RSS XML数据。
    维护一个 {Feed URL：状态} 字典，用户可以调用对应接口查询Feed的状态（未知，有效，无效，Busy等等）。
    提供另一个接口，清除Feed状态字典。
WebAPI则支持提交一个Feeds字典，同样更新到{Feed URL：状态} 字典，通过另一个wei api查询状态。
    同时给出调用该服务的测试代码。
命令行则接受一个url，同步验证其是否能得到一个合法的RSS XML数据，返回结果并退出。
界面则使用pyqt5，左侧输入单个链接或json，右侧则以列表显示每个feed链接的状态，feed状态的更新必须是异步的。
    用户可以勾选列表项，列表下方根据勾选项目动态生成json。程序提供“全选/全不选/仅选有效Feed/清除无效Feed”功能
如果不带命令行参数，默认启动界面，如果参数为url，则使用命令行。你来指定一个参数让可以启动web api服务。
将上面的功能实现在同一个py文件里，注意每个功能的独立性，比如pyqt5不存在不影响命令行的使用。

接受的JSON的格式如下：
"""
{
  "other keys just ignore": "........",
  "feeds": {
    "feed name 1": "feed url 1",
    "feed name 2": "feed url 2",
  }
}
"""

其中：
    列表要分列显示，列包括：Feed Name, Feed URL, Status
    动态生成的json项为  Feed Name: Feed URL，而非状态。





这个函数会遇到403的问题，我想，既然是验证RSS，那么正确的方法应该是利用标准的RSS请求来获取RSS资源，而非直接request爬取。
请帮我改造这段代码，在保持现有逻辑的前提下，使用标准的RSS请求来验证feed是否有效。






我使用feedparser库拉取并分析rss feed，但我发现该库反爬能力太弱，以致于对于很多feed根本拉取不到。
所以我希望通过无头浏览器拉取内容，再通过feedparser分析feed内容。
请帮我实现一个基于无头浏览器的强大网页内容获取程序，要求支持包括socks5在内的代理。





帮我实现一个parse_feed函数，参数是从rss feed返回的内容（content），你需要使用合适的库和工具对其进行解析。
该程序需要应对各种复杂和错误的情况，并且对每rss中的每一条数据，重新组织和返回关键字段的内容（请想一下rss的关键信息有哪些）。
注意不要出现语法错误和示实现的引用，不要简单的拼接代码，而是分析其功能和适用性，将其融合到自己的代码中。





我先给出requirements.txt中的内容，再给出所有python文件中import的内容，
帮我检查requirements.txt中是否有多余的依赖，或者缺失的依赖，最终输出完整且移除多余引用的requirements.txt内容。

requirements.txt :
"""
"""

Imports :
"""
"""







作为一个专业的python程序员，接下来你需要使用简洁及优雅的代码实现我的需求，所有函数必须有标准的英文注释，并且考虑错误处理及可测试性。

我希望通过url抓取一个网页，其中url来自RSS feed的XML文章描述。
帮我实现一个能应对大多数反爬情况的网页抓取模块，需要支持包括socks5在内的代理，但不需要考虑过于复杂的挑战的情况。
即尽可能地模拟浏览器浏览和渲染过程，需要考虑抓取的成功率，而不需要过多考虑抓取的效率。
需要考虑抓取函数可重入或抓取模块多实例化，以便多线程抓取。
请审视编写的代码，确定没有任何的引用未被实现。
在其它模块中使用了playwright，如果合适，你也可以考虑使用它。








我使用sync_playwright和requests实现不同模块来抓取网页，但它们的代理配置似乎不一样。

sync_playwright的格式是：

    proxy_config = {
        "server": "socks5://127.0.0.1:10808",
        "username": "",
        "password": ""
    }


而requests的格式是：


    proxies = {
        "http": "socks5://user:password@proxy_host:port",
        "https": "socks5://user:password@proxy_host:port"
    }


我希望它们能使用同样的格式，或者通过适配函数将格式转成一致。
注意要考虑到代理不需要认证，即username和password不存在的情况，一个函数要能同时处理有认证的情况和无认证的情况。
各个函数需要有标准英文注释，包括描述输入格式，输出格式。





针对 parse_proxy，to_playwright，to_requests，考虑各种情况，包括非法输入情况，为它们各自编写测试函数。
不需要使用任何测试框架，只需要使用assert进行断言即可。






在我的情报采集系统中，我想写一个prompt让AI给网络信息进行评分，分制为0 - 10分。
很显然，广告和完全无效的内容评分为0分。反之，诸如911和特朗普遇袭这类国际重大事件评为10分。
但对于中间具体的评分标准，我还没有思路。能不能帮助我制定关于情报价值评分标准的描述？如果有现成的成熟通用标准，那么最好。







帮我实现一个名为IntelligenceHub的类，它采用多线程实现以下功能：
1. 信息收集模块：提供WEB API，接受POST请求，参数是DICT，除UUID必须有之外，其余字段皆不限制，将收集信息放入待处理队列中。
    同时提供调用该WEB API的本地API（proxy）
2. 信息处理模块：从队列中获取待处理数据，通过POST请求调用WEB接口。该接口参数同样是DICT，除UUID外其余字段皆不限制。
   它将采用UUID作为消息的唯一标识，当消息异步处理结束后，该UUID将用来匹配之前的请求。
   如果一个数据在一定时间内未得到处理，将作为超时处理，将其放回待处理列表中，根据重试次数决定它的优先度。
3. 信息处理反馈模块
    提供WEB API，接受处理之后的消息，使用UUID匹配源消息。将源消息和处理后的消息一起放入队列中给后处理和归档模块。
    同时提供调用该WEB API的本地API（proxy）。
4. 后处理和归档模块：
    归档：将处理后的结果放入mongodb中，并建立指向它的向量数据库索引。
    后处理暂时未定，有可能采用插件的方式实现。
以上只是一个大概的框架，细节部分请加上TODO标记，由后续逐步实现。
一定一定要注意并发场景及效率，不要出现资源冲突的情况。



我实现了一个名为IntelligenceHub的类，它采用多线程实现以下功能：
1. 信息收集模块：提供WEB API，接受POST请求，参数是DICT，除UUID必须有之外，其余字段皆不限制，将收集信息放入待处理队列中。
    同时提供调用该WEB API的本地API（post_collected_intelligence）
2. 信息处理模块：从队列中获取待处理数据，通过POST请求调用WEB接口。该接口参数同样是DICT，除UUID外其余字段皆不限制。
   它将采用UUID作为消息的唯一标识，当消息异步处理结束后，该UUID将用来匹配之前的请求。
   如果一个数据在一定时间内未得到处理，将作为超时处理，将其放回待处理列表中，根据重试次数决定它的优先度。
3. 信息处理反馈模块
    提供WEB API，接受处理之后的消息，使用UUID匹配源消息。将源消息和处理后的消息一起放入队列中给后处理和归档模块。
    同时提供调用该WEB API的本地API（post_processed_intelligence）。
4. 后处理和归档模块：
    归档：将处理后的结果放入mongodb中，并建立指向它的向量数据库索引。
    后处理暂时未定，有可能采用插件的方式实现。

请一步步地分析该程序，帮我实现和改进你认为可以进一步实现或改进的地方。




请分析IntelligenceHub的功能和行为，并为IntelligenceHub编写包括Collector模拟, IntelligenceProcessor模拟在内的测试辅助模块。





基于它们编写测试用例，先测试整个处理流程，然后分条列出测试各种情况以及测试性能的测试用例简要描述。先不要输出测试代码。
先不要输出testcase的实现。












name	rss source
微软AI	https://blogs.microsoft.com/ai/feed/
微软亚研	https://www.msra.cn/feed
pytorch blog	https://pytorch.org/blog/
meta AI blog	https://ai.facebook.com/blog/
google ai blog	https://ai.googleblog.com/
OpenAI blog	https://openai.com/blog/
DeepMind blog	https://deepmind.com/blog
美团技术团队	https://tech.meituan.com/feed/
知乎每日精选	https://www.zhihu.com/rss
码农场	https://www.hankcs.com/feed/
Yong's Blog	https://yongyuan.name/blog/feed.xml
Jay Alammar's Blog	http://jalammar.github.io/
算法之道	https://www.deeplearn.me/
Andrej Karpathy blog	https://karpathy.github.io/
Sebastian Ruder	https://ruder.io/rss/index.rss
Jason Brownlee	https://machinelearningmastery.com/blog/
distill	https://distill.pub/
BAIR Blog	https://bair.berkeley.edu/blog/
colah's blog	http://colah.github.io/
inFERENCe	https://www.inference.vc/
Pete Warden	https://petewarden.com/
Jeremy Jordan	https://www.jeremyjordan.me/
Graduate Descent	http://timvieira.github.io/blog/
Lil'Log	https://lilianweng.github.io/
NVIDIA Blog	https://blogs.nvidia.com/
Eric Jang	https://blog.evjang.com/feeds/posts/default
Christabella Irwanto	https://bella.cc/blog/
Jonty Sinai	https://jontysinai.github.io/
Lei Mao	https://leimao.github.io/blog/
Amit Chaudhary	https://amitness.com/
Sander Dieleman	https://benanne.github.io/
Guillaume Genthial	https://guillaumegenthial.github.io/
Causal Analysis in Theory and Practice	http://causality.cs.ucla.edu/blog/
Machine learning and learning theory research	https://hunch.net/
ShusenWang	https://www.youtube.com/c/ShusenWang/videos
offconvex	http://www.offconvex.org/
pythonspeed	https://pythonspeed.com/
huggingface	https://huggingface.co/blog
wandb	https://wandb.ai/fully-connected
Han Xiao	https://hanxiao.io/archives/


https://www.federalreserve.gov/feeds/feeds.htm


