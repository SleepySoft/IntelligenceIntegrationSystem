作为一个专业的python程序员，接下来你需要使用简洁及优雅的代码实现我的需求，所有函数必须有标准的英文注释，并且考虑错误处理及可测试性。理解请回复“OK”。


我希望实现一个类，它能够根据rss订阅列表，对其中的每一个rss feed，下载其中所有的文章内容，包括：
1. 文章标题
2. 发布时间
3. 文章链接
4. 文章内容
其中rss feed列表会通过读取json文件输入，json格式如下：
```
{
  "theme": "科技",
  "focus": "AI",
  "prompt": "",
  "feeds": {
    "青柠学术": "https://qnscholar.github.io//feed.xml",
    "MOOC中国": "https://www.cmooc.com/feed",
    "知乎日报": "https://feeds.feedburner.com/zhihu-daily",
    "知乎每日精选": "https://www.zhihu.com/rss"
  }
}
```
请将读取json文件、遍历feeds、对每个feed下载、对feed中其中一篇文章的链接下载各写在不同函数里。

对于上面的需求有不清楚之处清进一步询问，如果有更多的建议也请提出。




我希望将正文转换为markdown格式以减少复杂度。




在这个函数里面，调用另一个函数，用来将不同feed的文章分文件夹存储为<标题>.md文件。注意需要保证目录存在，且<标题>作为文件名有效，否则将其中非法字符进行清洗。
不要在最后一起写入，而是拉取一个写一个，这是为了防止中途出错。另外请考虑这么一种机制：成功拉取文章后记录一个“URL” - "文件"的持久性存储表，在每次拉取时，不要拉取已经拉取过的文章。





帮我写一个FeedsValidation的python脚本，提供本地API，WebAPI，命令行和界面四种界面。
本地API行为如下：
    传入一个URL，同步验证其是否能得到一个合法的RSS XML数据。
    传入一个Feeds字典，异步验证其中每一个URL能得到一个合法的RSS XML数据。
    维护一个 {Feed URL：状态} 字典，用户可以调用对应接口查询Feed的状态（未知，有效，无效，Busy等等）。
    提供另一个接口，清除Feed状态字典。
WebAPI则支持提交一个Feeds字典，同样更新到{Feed URL：状态} 字典，通过另一个wei api查询状态。
    同时给出调用该服务的测试代码。
命令行则接受一个url，同步验证其是否能得到一个合法的RSS XML数据，返回结果并退出。
界面则使用pyqt5，左侧输入单个链接或json，右侧则以列表显示每个feed链接的状态，feed状态的更新必须是异步的。
    用户可以勾选列表项，列表下方根据勾选项目动态生成json。程序提供“全选/全不选/仅选有效Feed/清除无效Feed”功能
如果不带命令行参数，默认启动界面，如果参数为url，则使用命令行。你来指定一个参数让可以启动web api服务。
将上面的功能实现在同一个py文件里，注意每个功能的独立性，比如pyqt5不存在不影响命令行的使用。

接受的JSON的格式如下：
"""
{
  "other keys just ignore": "........",
  "feeds": {
    "feed name 1": "feed url 1",
    "feed name 2": "feed url 2",
  }
}
"""

其中：
    列表要分列显示，列包括：Feed Name, Feed URL, Status
    动态生成的json项为  Feed Name: Feed URL，而非状态。





这个函数会遇到403的问题，我想，既然是验证RSS，那么正确的方法应该是利用标准的RSS请求来获取RSS资源，而非直接request爬取。
请帮我改造这段代码，在保持现有逻辑的前提下，使用标准的RSS请求来验证feed是否有效。






我使用feedparser库拉取并分析rss feed，但我发现该库反爬能力太弱，以致于对于很多feed根本拉取不到。
所以我希望通过无头浏览器拉取内容，再通过feedparser分析feed内容。
请帮我实现一个基于无头浏览器的强大网页内容获取程序，要求支持包括socks5在内的代理。





帮我实现一个parse_feed函数，参数是从rss feed返回的内容（content），你需要使用合适的库和工具对其进行解析。
该程序需要应对各种复杂和错误的情况，并且对每rss中的每一条数据，重新组织和返回关键字段的内容（请想一下rss的关键信息有哪些）。
注意不要出现语法错误和示实现的引用，不要简单的拼接代码，而是分析其功能和适用性，将其融合到自己的代码中。





我先给出requirements.txt中的内容，再给出所有python文件中import的内容，
帮我检查requirements.txt中是否有多余的依赖，或者缺失的依赖，最终输出完整且移除多余引用的requirements.txt内容。

requirements.txt :
"""
"""

Imports :
"""
"""







作为一个专业的python程序员，接下来你需要使用简洁及优雅的代码实现我的需求，所有函数必须有标准的英文注释，并且考虑错误处理及可测试性。

我希望通过url抓取一个网页，其中url来自RSS feed的XML文章描述。
帮我实现一个能应对大多数反爬情况的网页抓取模块，需要支持包括socks5在内的代理，但不需要考虑过于复杂的挑战的情况。
即尽可能地模拟浏览器浏览和渲染过程，需要考虑抓取的成功率，而不需要过多考虑抓取的效率。
需要考虑抓取函数可重入或抓取模块多实例化，以便多线程抓取。
请审视编写的代码，确定没有任何的引用未被实现。
在其它模块中使用了playwright，如果合适，你也可以考虑使用它。








我使用sync_playwright和requests实现不同模块来抓取网页，但它们的代理配置似乎不一样。

sync_playwright的格式是：

    proxy_config = {
        "server": "socks5://127.0.0.1:10808",
        "username": "",
        "password": ""
    }


而requests的格式是：


    proxies = {
        "http": "socks5://user:password@proxy_host:port",
        "https": "socks5://user:password@proxy_host:port"
    }


我希望它们能使用同样的格式，或者通过适配函数将格式转成一致。
注意要考虑到代理不需要认证，即username和password不存在的情况，一个函数要能同时处理有认证的情况和无认证的情况。
各个函数需要有标准英文注释，包括描述输入格式，输出格式。





针对 parse_proxy，to_playwright，to_requests，考虑各种情况，包括非法输入情况，为它们各自编写测试函数。
不需要使用任何测试框架，只需要使用assert进行断言即可。






在我的情报采集系统中，我想写一个prompt让AI给网络信息进行评分，分制为0 - 10分。
很显然，广告和完全无效的内容评分为0分。反之，诸如911和特朗普遇袭这类国际重大事件评为10分。
但对于中间具体的评分标准，我还没有思路。能不能帮助我制定关于情报价值评分标准的描述？如果有现成的成熟通用标准，那么最好。







帮我实现一个名为IntelligenceHub的类，它采用多线程实现以下功能：
1. 信息收集模块：提供WEB API，接受POST请求，参数是DICT，除UUID必须有之外，其余字段皆不限制，将收集信息放入待处理队列中。
    同时提供调用该WEB API的本地API（proxy）
2. 信息处理模块：从队列中获取待处理数据，通过POST请求调用WEB接口。该接口参数同样是DICT，除UUID外其余字段皆不限制。
   它将采用UUID作为消息的唯一标识，当消息异步处理结束后，该UUID将用来匹配之前的请求。
   如果一个数据在一定时间内未得到处理，将作为超时处理，将其放回待处理列表中，根据重试次数决定它的优先度。
3. 信息处理反馈模块
    提供WEB API，接受处理之后的消息，使用UUID匹配源消息。将源消息和处理后的消息一起放入队列中给后处理和归档模块。
    同时提供调用该WEB API的本地API（proxy）。
4. 后处理和归档模块：
    归档：将处理后的结果放入mongodb中，并建立指向它的向量数据库索引。
    后处理暂时未定，有可能采用插件的方式实现。
以上只是一个大概的框架，细节部分请加上TODO标记，由后续逐步实现。
一定一定要注意并发场景及效率，不要出现资源冲突的情况。



我实现了一个名为IntelligenceHub的类，它采用多线程实现以下功能：
1. 信息收集模块：提供WEB API，接受POST请求，参数是DICT，除UUID必须有之外，其余字段皆不限制，将收集信息放入待处理队列中。
    同时提供调用该WEB API的本地API（post_collected_intelligence）
2. 信息处理模块：从队列中获取待处理数据，通过POST请求调用WEB接口。该接口参数同样是DICT，除UUID外其余字段皆不限制。
   它将采用UUID作为消息的唯一标识，当消息异步处理结束后，该UUID将用来匹配之前的请求。
   如果一个数据在一定时间内未得到处理，将作为超时处理，将其放回待处理列表中，根据重试次数决定它的优先度。
3. 信息处理反馈模块
    提供WEB API，接受处理之后的消息，使用UUID匹配源消息。将源消息和处理后的消息一起放入队列中给后处理和归档模块。
    同时提供调用该WEB API的本地API（post_processed_intelligence）。
4. 后处理和归档模块：
    归档：将处理后的结果放入mongodb中，并建立指向它的向量数据库索引。
    后处理暂时未定，有可能采用插件的方式实现。

请一步步地分析该程序，帮我实现和改进你认为可以进一步实现或改进的地方。




请分析IntelligenceHub的功能和行为，并为IntelligenceHub编写包括Collector模拟, IntelligenceProcessor模拟在内的测试辅助模块。





基于它们编写测试用例，先测试整个处理流程，然后分条列出测试各种情况以及测试性能的测试用例简要描述。先不要输出测试代码。
先不要输出testcase的实现。






processor_thread是一个IO线程，因为等待超时所以会成为性能瓶颈。我希望按照CPU数除以2创建一个线程池，由该线程池来进行数据处理。
注意不要频繁创建和退出线程，而是通过信号里让线程等待数据，并且不影响检查退出标记。相关代码如下：








理解下面一段代码，增加一个类，功能如下：
    提供一个接口，参数与load_json_config()一样，名字请根据功能自行推测。
    调用该函数在读取json配置后，会创建一个线程以polling_interval周期循环执行processor.process_all_feeds()。
    要注意使用线程退出标志让线程有优雅退出的机会，以文件名为索引可以找到并管理线程。
    如果发现参数中的文件名已存在线程列表中，如果指定force，则关闭旧线程开启新线程，否则返回错误并忽略。
    该类同时还要提供统计信息获取接口，并注意多线程资源访问问题。







写一个函数，接受两个参数：key属性表，待检查的dict。
key属性表类似下面的例子：

COLLECTOR_DATA_FIELDS = {
    'UUID': 'M',
    'source': 'O',
}

基中M代表MUST，O代表OPTIONAL。函数返回 (MUST字段的缺失数，OPTIONAL字段的存在数，不在属性表中的字段数)






python有没有可能实现这样一种机制：
当我向特定文件夹放入一个python文件时，这个新加入文件会被服务识别并从指定的函数启动。
当我从这个文件夹中移除一个文件时，服务会在该文件对应的任务执行完毕后移除该任务（移除文件对程序运行本身没有影响）。
请仔细分析这个需求，分析其中的问题及实现可能性，给出上面提到的“服务”程序的代码。







python有没有这样的机制：开辟一个有P个线程的线程池，执行T个任务（T可能大于P）。
里面的任务不会发生抢占







下面这段代码，由于一个sql连接无法在线程间共享，因此我希望将整个save_content做成异步的。
即在该单例初始化时创建一个线程，调用save_content时仅把数据放入队列中，并通知线程做后续操作。
该类需提供优雅退出线程的方法，并且注意线程安全问题。



使用crawl4ai实现以下接口，要求能对抗大部分常用的反爬手段，并且提供分析抓取失败的诊断手段。


class ProxyConfig(TypedDict):
    http: str
    https: str


class ScraperResult(TypedDict):
    content: str
    errors: List[str]


def fetch_content(
    url: str,
    timeout_ms: int,
    proxy: Optional[ProxyConfig] = None,
    **kwargs
) -> ScraperResult:




写一段python程序，使用openai的接口调用硅基流动的API接口（是不是理论上可以支持所有openai like接口？）。
token可以在构造时从外界传入，如果没有，则去环境变量中获取。
可以选择采用同步或异步的方式获取结果。
所有函数都需要标准注释，且关键地方也需要注释说明意图和数据格式，所有注释必须为英文。













基于我们的对话上下文，请分析并理解我的需求，基于上面的AI接口，编写一个函数实现下面描述的功能：

输入：主prompt，结构化的数据（json，其中主要内容是content字段，其它比如title字段等等为可选），对话上下文（可选）
输出：按prompt输出的json格式。
函数功能：将输入的prompt和结构化数据传递给AI，等待输出。输出应当为json格式，将其转为dict输出。

主prompt示例如下：









程序中已通过以下代码初始化好了mongodb client：

"""
def _setup_mongo_db(self) -> bool:
    """初始化MongoDB连接并创建必要索引"""
    try:
        # 连接配置
        self.mongo_client = pymongo.MongoClient(
            self.mongo_db_uri,
            maxPoolSize=100,
            serverSelectionTimeoutMS=5000
        )

        # 验证连接
        self.mongo_client.admin.command('ping')
        self.db = self.mongo_client["intelligence_db"]
        self.archive_col = self.db["processed_data"]

        # 定义索引规范（名称、字段、选项）
        index_specs = [
            {
                "name": "created_at_idx",
                "keys": [("metadata.created_at", pymongo.ASCENDING)],
                "description": "基于创建时间的时间范围查询加速"
            },
            {
                "name": "vector_dim_idx",
                "keys": [("vector.dim", pymongo.ASCENDING)],
                "options": {
                    "partialFilterExpression": {"vector": {"$exists": True}},
                    "background": True  # 后台创建避免阻塞
                },
                "description": "向量维度查询优化（仅当存在vector字段时建立）"
            },
            {
                "name": "content_text_idx",
                "keys": [("raw_data.value", "text")],
                "options": {
                    "weights": {"raw_data.value": 5},
                    "default_language": "english",
                    "language_override": "language"
                },
                "description": "全文检索索引（字段权重优化）"
            }
        ]

        # 检查并创建索引
        existing_indexes = {idx["name"]: idx for idx in self.archive_col.list_indexes()}

        for spec in index_specs:
            index_name = spec["name"]
            index_model = pymongo.IndexModel(spec["keys"], **spec.get("options", {}))

            # 索引存在性检查
            if index_name in existing_indexes:
                existing = existing_indexes[index_name]
                # 检查索引定义是否一致
                if (existing["key"] == index_model.document['key'] and
                        existing.get("partialFilterExpression") == spec.get("options", {}).get(
                            "partialFilterExpression")):
                    logger.info(f"索引 {index_name} 已存在，跳过创建")
                    continue

                # 删除不一致的旧索引
                logger.warning(f"检测到不一致索引 {index_name}，重新创建...")
                self.archive_col.drop_index(index_name)

            # 创建新索引
            logger.info(f"创建索引 {index_name}：{spec['description']}")
            self.archive_col.create_indexes([index_model])

        return True
    except pymongo.errors.OperationFailure as e:
        logger.critical(f"MongoDB索引操作失败: {str(e)}")
        return False
    except Exception as e:
        logger.critical(f"MongoDB连接失败: {str(e)}")
        return False
"""


请写一个函数将dict写入该数据库。dict示例如下（省略部分数据）：

"""
C:\D\Code\git\IntelligenceIntegrationSystem\.venv\Scripts\python.exe -X pycache_prefix=C:\Users\yq1061170\AppData\Local\JetBrains\PyCharmCE2024.2\cpython-cache "C:/Program Files/JetBrains/PyCharm Community Edition 2024.2.3/plugins/python-ce/helpers/pydev/pydevd.py" --multiprocess --qt-support=auto --client 127.0.0.1 --port 50471 --file C:\D\Code\git\IntelligenceIntegrationSystem\Tools\IntelligenceAnalyzerProxy.py
Connected to pydev debugger (build 242.23339.19)
AI response spends 94.8275637626648 s
{
  "UUID": "55eebb04-1400-4b33-9800-7fdb43ad4689",
  "TIME": "2024-07-19",
  "LOCATION": "",
  "PEOPLE": [],
  "ORGANIZATION": [
    "Netflix",
    "Paramount",
    "Disney",
    "Amazon",
    "Corus",
    "Global TV",
    "",
    "Motion Picture Association-Canada"
  ],
  "EVENT_BRIEF": "",
  "EVENT_TEXT": ",
  "RATE": {
    "战略相关性": 6,
    "国际关联度": 4,
    "金融影响力": 6,
    "政策关联度": 8,
    "科技前瞻性": 2,
    "投资价值": 6,
    "内容准确率": 8
  },
  "IMPACT": "",
  "TIPS": "流媒体与传统媒体就加拿大内容资金义务产生政策博弈"
}

Process finished with exit code 0

"""




作为一个资深专业的python程序员，帮我实现一个mongodb存储类，要求如下：
1. 存储通用的dict，而不要去管dict中的键值。
2. 提供基本的查询接口。
3. 良好的可配置性，如服务地址，数据库名，登录认证等等，参数配置需配上详细说明。
4. 多线程访问安全，保证资源不泄露。
5. 适度通用的优化，以增加访问速度，优化需要配上详细说明。
6.注释必须标准且为英文，所有文字都需要是标准专业的英文。




使用并改造以下的类，实现以下功能：
1. 增加一个update的方法，指定一个或多个字段以及值（dict），并指定需要更新或增加的字段（dict），更新对应记录。
2. 增加一个查询接口，指定，指定一个或多个字段以及值（dict），返回记录的dict列表（转成dict，而非原始的查询记录）











使用python，基于faiss实现一个向量数据库类，提供以下功能：

1. 能指定存储路径或数据库名
2. 指定一个id(str)和一段文本，为文本生成相应的向量并与id关联存储
3. 支持查询功能，提供一段文本和topN，返回匹配的id list。

要求使用合适的尽可能简单和轻量的实现，不要使用过于重型和收费的库。









帮我检查这段代码，一步步分析代码的功能和作者的用意，从以下角度思考：
1. 代码是否有问题或漏洞
2. 代码是否健壮
3. 代码是否有可以改进的地方
4. 考虑多线程的问题
综合以上分析，最终给出你的改进建议。





现在（2025年）python接入微信聊天（非小程序）的方案有哪些？分别有什么限制和特点？


有什么开源或开放的IM软件，满足以下需求：
1. 包含Android客户端和PC客户端
2. 支持python接入
3. 国内易于访问






使用python写一个RSS的发布程序，一方面接收远程或本地的调用请求，根据提供的信息建立RSS信息库，另一方面将这个信息按指定端口提供标准的RSS Feed。
注意要考虑到信息的滚动更新，并且需要考虑到在被调用和进行推送之前，启动时的初始Feed列表如何建立。究竟是使用推送的方式，还是使用类似代理的方式，由你统筹考虑后决定。
注意核心代码中不要涉及任何有关webservice的代码，核心类仅提供API供webservice调用。webservice在测试代码中实现，并独立于RSS核心类。
注意所有文本，包括注释需要使用英文。


MongoDB里存储了一系列dict数据，key的描述如下：

"""
    UUID: str
    TIME: str | None = None
    LOCATION: list | None = None
    PEOPLE: list | None = None
    ORGANIZATION: list | None = None
    EVENT_BRIEF: str | None = None
    EVENT_TEXT: str | None = None
    RATE: dict | None = {}
    IMPACT: str | None = None
    TIPS: str | None = None
"""

基于以上信息，请理解并根据以下函数的参数，构造查询

"""
    def query_intelligence(self,
                           *,
                           db: str = 'cache',
                           period:      Optional[Tuple[datetime.date, datetime.date]] = None,
                           locations:   Optional[str, List[str]] = None,
                           peoples:     Optional[str, List[str]] = None,
                           organizations: Optional[str, List[str]] = None,
                           keywords: Optional[str] = None
                           ):
        pass
"""

另一个涉及查询的参考代码如下：

"""
    def _load_unarchived_data(self):
        """Load unarchived data into a queue."""
        if not self.mongo_db_cache:
            return

        try:
            # 1. Build query
            query = {APPENDIX_ARCHIVED_FLAG: {"$exists": False}}

            # 2. Stream processing
            cursor = self.mongo_db_cache.collection.find(query)
            for doc in cursor:
                # Convert ObjectId to string
                doc['_id'] = str(doc['_id'])

                # 3. Handle queue with timeout
                try:
                    self.original_queue.put(doc, block=True, timeout=5)
                except queue.Full:
                    logger.error("Queue full, failed to add document")
                    break

            logger.info(f'Previous unprocessed data loaded, item count: {self.original_queue.qsize()}')

        except pymongo.errors.PyMongoError as e:
            logger.error(f"Database operation failed: {str(e)}")
"""





















使用python写一个函数 default_article_render() ，传入下面这样的一个dict，内容描述如下。要求将其内容组织为易于阅读且美观的HTML，便于用户浏览。其中使用的css最好是网上通用的在线格式。

{
  "UUID": "将输入的UUID原封不动地放在这里",
  "INFORMANT": "如果输入包含URL，则将输入的URL放在这里，否则查找来源信息，如果没有相关信息，则为空"
  "TIME": "YYYY-MM-DD格式，无则null",
  "LOCATION": ["文章主体中涉及的 国家/省/市/具体地址，无则为空列表"],
  "PEOPLE": ["文章主体中涉及的 姓名列表, 无则为空列表"],
  "ORGANIZATION": ["文章主体中涉及的 国家/公司/宗教/机构/组织名称等, 无则为空列表"],
  "EVENT_TITLE": "20字内描述核心内容的标题",
  "EVENT_BRIEF": "50字内描述核心事实",
  "EVENT_TEXT": "去除广告及主题无关信息后，干净简洁的文章内容",
  "RATE": {
    "战略相关性": "0-10分偶数",
    "国际关联度": "0-10分偶数",
    "金融影响力": "0-10分偶数",
    "政策关联度": "0-10分偶数",
    "科技前瞻性": "0-10分偶数",
    "投资价值": "0-10分偶数",
    "内容准确率": "0-10分偶数",
  },
  "IMPACT": "该事件可能的影响及推论，主要是对资产的影响，如无明显影响，则为空",
  "TIPS": "你给用户的备注信息，不要超过50个字"
}






实现以下函数：

def dump_text(self) -> str:
    pass

这个函数用以将 self.config_data 这个dict dump为易读的文本。
关键点在于：self.config_data会嵌套其它dict，对于嵌套的dict，输出的key以"."分隔。例如：

```
{
  "intelligence_hub": {

    "ai_service": {
      "url": "http://localhost:11434",
      "token": "SleepySoft",
      "model": "qwen3:14b"
    }
}
```

dump示例为：

intelligence_hub.ai_service.url: "http://localhost:11434"
intelligence_hub.ai_service.token: "SleepySoft"
intelligence_hub.ai_service.model: "qwen3:14b"

你可以自由调整对齐和分隔符等使得输出更加好看易读。







我有以下prompt，但其在规模较小的模型上表现不稳定，请分析原因和优化以下的prompt，使其表述更精辟和精确，同时在较小的模型上有同样好的表现。
要求检查原prompt中的错误，细化"RATE"中的评分标准，给出更典型的输出示例，以准确指导模型准确输出内容。


DEFAULT_ANALYSIS_PROMPT = """# 角色设定
你是一个专业情报分析师，需对输入文本进行结构化解析与价值评分

注意：对于文学性，介绍性等没有情报价值，或情报价值较低的文章，不要犹豫，直接返回以下json字符串：

{
  "UUID": "将输入的UUID原封不动地放在这里"
}

排除没有情报价值的文章后，按下面的要对文章进行分析。

# 处理流程
1. 按字段顺序提取五要素
2. 生成信息浓缩的标题
3. 按评分标准进行维度评分
4. 输出严格符合JSON格式
5. 无论原文章是什么语言，处理后的结果必须是中文

# 输出要求
{
  "UUID": "将输入的UUID原封不动地放在这里",
  "INFORMANT": "如果输入包含URL，则将输入的URL放在这里，否则查找来源信息，如果没有相关信息，则为空"
  "TIME": "YYYY-MM-DD格式，无则null",
  "LOCATION": ["文章主体中涉及的 国家/省/市/具体地址，无则为空列表"],
  "PEOPLE": ["文章主体中涉及的 姓名列表, 无则为空列表"],
  "ORGANIZATION": ["文章主体中涉及的 国家/公司/宗教/机构/组织名称等, 无则为空列表"],
  "EVENT_TITLE": "20字内描述核心内容的标题",
  "EVENT_BRIEF": "50字内描述核心事实",
  "EVENT_TEXT": "去除广告及主题无关信息后，干净简洁的文章内容。对于翻译的内容，注意语言流畅及本地化，禁止翻译腔",
  "RATE": {
    "战略相关性": "0-10分偶数",
    "国际关联度": "0-10分偶数",
    "金融影响力": "0-10分偶数",
    "政策关联度": "0-10分偶数",
    "科技前瞻性": "0-10分偶数",
    "投资价值": "0-10分偶数",
    "内容准确率": "0-10分偶数",
  },
  "IMPACT": "该事件可能的影响及推论，主要是对资产的影响，如无明显影响，则为空",
  "TIPS": "你给用户的备注信息，不要超过50个字"
}

# 评分细则（仅取整数分）
## 战略相关性（25%权重）
10分：改变国际力量格局（如战争、科技封锁）
8分：引发区域经济重构（如自贸区建立）
6分：国家战略级政策发布（国家级别）
4分：行业标准变更（行业级别）
2分：地方性政策

## 国际关联度（20%权重）
10分：涉及3+主要经济体
8分：双边协议影响供应链
6分：单边政策全球波及
4分：跨国企业重大调整

## 金融影响力（15%权重）
10分：引发全球市场剧烈波动
8分：改变大宗商品定价
6分：资本流动规则变更
4分：融资环境重大变化

## 政策关联度（15%权重）
10分：国家级产业政策突破
8分：监管框架重构
6分：税收/补贴调整
4分：地方试点推广

## 科技前瞻性（10%权重）
10分：颠覆性技术突破
8分：卡脖子技术进展
6分：专利格局变化
4分：科研投入转向

## 投资价值（10%权重）
10分：创造新产业赛道
8分：改变估值逻辑
6分：影响资产配置
4分：催生金融产品

## 内容准确率（5%权重）
10分：多方权威验证
8分：含原始数据
6分：逻辑自洽推测
4分：部分数据存疑

# 输出示例
输入："
- UUID: 218bd628-33c4-401a-b081-313f9da653a9
- title: 新华社消息
- authors: ['胡锡桧']
- pub_time: published
- informant: https://www.cbc.ca/news/science

## 正文内容
2024年7月19日，中美在APEC峰会达成人工智能芯片出口管制协议，涉及英伟达、AMD等企业
"

输出：
{
  "UUID": "218bd628-33c4-401a-b081-313f9da653a9",
  "INFORMANT": "https://www.cbc.ca/news/science",
  "TIME": "2024-07-19",
  "LOCATION": "APEC峰会",
  "PEOPLE": [],
  "ORGANIZATION": ["中国","美国","英伟达","AMD"],
  "EVENT_TITLE": "中美达成AI芯片出口管制协议",
  "EVENT_BRIEF": "中美在APEC峰会上达成AI芯片出口管制协议",
  "EVENT_TEXT": "2024年7月19日，中美在APEC峰会达成人工智能芯片出口管制协议，涉及英伟达、AMD等企业",
  "RATE": {
    "战略相关性":6,
    "国际关联度":8,
    "金融影响力":8,
    "政策关联度":8,
    "科技前瞻性":6,
    "投资价值":8,
    "内容准确率":10
  }
  "IMPACT": "该事件有可能影响AI相关芯片的价格",
  "TIPS": ""
}

# 异常处理
- 存在矛盾信息时取最低维度分
- 广告内容所有维度均计0分
- 无法判断的字段用null表示
"""







我先给出一份requirements.txt中的内容，里面会列出依赖的库，但并没有具体的版本信息。
接下来我会给出pip freeze的输出，毫无疑问，其中会包含详细的版本信息，但也会包含不需要的库，或间接依赖的库。
我希望：
1. 参考 requirements.txt ，并且根据 pip freeze的输出，给出一份精简但包含正确版本信息的 requirements.txt 信息。
2. 对于输出的 requirements.txt 中的每一项，加上对齐的注释，说明其主要用途，以及有需要的，任何附加说明。
3. 输出的 requirements.txt 最好能注意分组和美观性。

```requirements.txt
lxml
chardet
fastapi
uvicorn
cryptography
bs4~=0.0.1
Flask>=2.0.3
PyQt5>=5.15.6
tenacity>=8.2.3
feedparser==6.0.10
markdownify~=1.1.0
brotli
requests[socks]>=2.26.0
pymongo
pytest
werkzeug
html2text
watchdog
pytest-asyncio
tldextract

urllib3>=1.26.9,<1.27
beautifulsoup4>=4.12
playwright>=1.49.0					            # Then run: playwright install chromium
crawl4ai==0.6.3

numpy
pandas

# transformers==4.36.0
# sentence-transformers
# hnswlib

faiss-cpu           # Windows
# faiss-gpu           # Linux/Mac with NVIDIA card
huggingface_hub[hf_xet]


typing_extensions
tortoise-orm

PyRSS2Gen
```


```pip freeze
aiofiles==24.1.0
aiohappyeyeballs==2.6.1
aiohttp==3.12.2
aiosignal==1.3.2
aiosqlite==0.21.0
annotated-types==0.7.0
anyio==4.9.0
async-timeout==5.0.1
attrs==25.3.0
beautifulsoup4==4.13.4
blinker==1.9.0
Brotli==1.1.0
bs4==0.0.2
certifi==2025.4.26
cffi==1.17.1
chardet==5.2.0
charset-normalizer==3.4.2
click==8.2.1
colorama==0.4.6
Crawl4AI==0.6.3
cryptography==45.0.3
cssselect==1.3.0
distro==1.9.0
dnspython==2.7.0
exceptiongroup==1.3.0
faiss-cpu==1.11.0
fake-http-header==0.3.5
fake-useragent==2.2.0
fastapi==0.115.12
feedparser==6.0.10
filelock==3.18.0
Flask==3.1.1
frozenlist==1.6.0
fsspec==2025.5.1
greenlet==3.2.2
h11==0.16.0
hnswlib==0.8.0
html2text==2025.4.15
httpcore==1.0.9
httpx==0.28.1
httpx-aiohttp==0.1.4
huggingface-hub==0.32.2
humanize==4.12.3
idna==3.10
importlib_metadata==8.7.0
iniconfig==2.1.0
iso8601==2.1.0
itsdangerous==2.2.0
Jinja2==3.1.6
jiter==0.10.0
joblib==1.5.1
jsonschema==4.24.0
jsonschema-specifications==2025.4.1
litellm==1.71.1
lxml==5.4.0
markdown-it-py==3.0.0
markdownify==1.1.0
MarkupSafe==3.0.2
mdurl==0.1.2
mpmath==1.3.0
multidict==6.4.4
networkx==3.4.2
nltk==3.9.1
numpy==2.2.6
openai==1.82.0
packaging==25.0
pandas==2.2.3
pillow==10.4.0
playwright==1.52.0
pluggy==1.6.0
propcache==0.3.1
psutil==7.0.0
pycparser==2.22
pydantic==2.11.5
pydantic_core==2.33.2
pyee==13.0.0
Pygments==2.19.1
pymongo==4.13.0
pyOpenSSL==25.1.0
pyperclip==1.9.0
pypika-tortoise==0.6.1
PyQt5==5.15.11
PyQt5-Qt5==5.15.2
PyQt5_sip==12.17.0
PyRSS2Gen==1.1
PySocks==1.7.1
pytest==8.3.5
pytest-asyncio==1.0.0
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
pytz==2025.2
PyYAML==6.0.2
rank-bm25==0.2.2
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
requests-file==2.1.0
rich==14.0.0
rpds-py==0.25.1
safetensors==0.5.3
scikit-learn==1.6.1
scipy==1.15.3
sentence-transformers==3.0.1
sgmllib3k==1.0.0
six==1.17.0
sniffio==1.3.1
snowballstemmer==2.2.0
soupsieve==2.7
starlette==0.46.2
sympy==1.14.0
tenacity==9.1.2
tf-playwright-stealth==1.1.2
threadpoolctl==3.6.0
tiktoken==0.9.0
tldextract==5.3.0
tokenizers==0.15.2
tomli==2.2.1
torch==2.7.0
tortoise-orm==0.25.0
tqdm==4.67.1
transformers==4.36.0
typing-inspection==0.4.1
typing_extensions==4.13.2
tzdata==2025.2
urllib3==1.26.20
uvicorn==0.34.2
watchdog==6.0.0
Werkzeug==3.1.3
xxhash==3.5.0
yarl==1.20.0
zipp==3.22.0
```





分析并理解以下代码，做以下重构：

所有注释和文本皆为英文
query_intelligence增加条数限制参数
除非有重大问题，否则代码不要大改，否则难以比对你所更新的地方。







分析以下代码，增加几个函数，注意增加的函数注释及文本需要使用英文，下面提供的函数名仅供参考，请根据语义取更合适的名字：

get_intelligence_count_and_base: 获取当前所有的条目数据，同时返回首条索引，作为base
get_intelligence_by_base_offset: 将base，offset，count传入，获取以base为基准，offset后的count条数据（用做数据浏览，指定base以免新增条目引起跳动）







使用python写一个函数 default_article_list_render() ，传入下面这样的dict列表，dict内容描述如下。要求：
1. 将列表内容组织为易于阅读且美观的HTML，便于用户浏览。其中使用的css最好是网上通用的在线格式。
2. 标题显示超链接，格式为：'/intelligence/<string:intelligence_uuid>'。暂时使用相对路径url，点击在新页面打开该url。
3. 下方显示翻页按钮，当前页面的参数为“/intelligences?offset=100,count=20”，则上一页的url为“/intelligences?offset=80,count=20”，下一页为“/intelligences?offset=120,count=20”，以此类推。尽可能使用简单的方法实现这一功能。
4. 返回HTML，可以带有简单的样式表和脚本，不得过于复杂。
5. 注意该功能不使用前后端，如描述所示，所有页面由服务器生成。

{
  "UUID": "将输入的UUID原封不动地放在这里",
  "INFORMANT": "如果输入包含URL，则将输入的URL放在这里，否则查找来源信息，如果没有相关信息，则为空"
  "TIME": "YYYY-MM-DD格式，无则null",,
  "EVENT_TITLE": "20字内描述核心内容的标题",
  "EVENT_BRIEF": "50字内描述核心事实",
}







请分析以下函数，分析其各参数的类型（字符或列表，时间等）及关系（与/或，同时存在或互斥），写一个网页表单对其进行查询，并完善路由函数。

注意：
1. 所有文本需要使用英文，并考虑查询结果过多需要分页的问题。
2. 当访问/intelligences/query时，展示该网页。上半部分为表单（包括页码选择），下半部分为结果列表。所有内容由服务端生成，不要使用动态网页。

```

    def query_intelligence(
            self,
            *,
            period: Optional[Tuple[datetime.datetime, datetime.datetime]] = None,
            locations: Optional[Union[str, List[str]]] = None,
            peoples: Optional[Union[str, List[str]]] = None,
            organizations: Optional[Union[str, List[str]]] = None,
            keywords: Optional[str] = None,
            skip: Optional[int] = None,
            limit: Optional[int] = None
    ) -> List[dict]:
        """Execute intelligence query

        Args:
            period: UTC time range (start, end)
            locations: Location ID(s) (str or str list)
            peoples: Person ID(s) (str or str list)
            organizations: Organization ID(s) (str or str list)
            keywords: Full-text keywords
            skip: Number of documents to skip
            limit: Maximum number of results to return

        Returns:
            List of matching intelligence documents
        """
        # Get specified database collection
        collection = self.__mongo_db.collection

        try:
            # Build MongoDB query
            query = self.build_intelligence_query(
                period=period,
                locations=locations,
                peoples=peoples,
                organizations=organizations,
                keywords=keywords
            )

            # Execute query and return results with limit
            return self.execute_query(collection, query, skip=skip, limit=limit)

        except pymongo.errors.PyMongoError as e:
            logger.error(f"Intelligence query failed: {str(e)}")
            return []
        except Exception as e:
            logger.error(f"Intelligence query error: {str(e)}", stack_info=True)
            return []


    def build_intelligence_query(
            self,
            period: Optional[Tuple[datetime.datetime, datetime.datetime]] = None,
            locations: Optional[Union[str, List[str]]] = None,
            peoples: Optional[Union[str, List[str]]] = None,
            organizations: Optional[Union[str, List[str]]] = None,
            keywords: Optional[str] = None
    ) -> dict:
        query_conditions = []

        if period:
            query_conditions.append(self.build_time_condition(*period))

        if locations:
            query_conditions.append(self.build_list_condition("LOCATION", locations))

        if peoples:
            query_conditions.append(self.build_list_condition("PEOPLE", peoples))

        if organizations:
            query_conditions.append(self.build_list_condition("ORGANIZATION", organizations))

        if keywords:
            query_conditions.append(self.build_keyword_or_condition(keywords))

        return {"$and": query_conditions} if query_conditions else {}
```


```
@self.app.route('/intelligences/query', methods=['GET', 'POST'])
def intelligences_query_api():
    # TODO: self.intelligence_hub.query_intelligence(...)
    pass
```





帮我写一个文章管理的功能，当有请求到来时，检查对应的markdown文件，并生成对应的html文件到template目录下。

为了避免反复生成，在转换成html时会记录markdown文件的hash，如果hash相同且对应html文件存在，则不会重新生成。

为了避免和手写的template混淆，生成的html位于generated子目录下，生成文件之前要确保该目录存在。










mongodb的记录中有RATE字段，内容为{ "评分维度": "评分" }。评分维度非固定，评分统一为0 - 10分。
我想筛选出只要一个评分维度大于阈值N分的记录，这个查询语句应该怎么写？









有没有python的log，分类浏览，统计，网页仪表盘的整合解决方案。






分析我的意图并帮我完善代码，注意注释文本都需要使用英文：

class CrawlStatistics:
    # TODO: Singleton with multi-thread protecting

    def __init__(self):
        self._counter_log_lock = threading.Lock()
        self._counter_log_record = {}

        self._sub_item_log_lock = threading.Lock()
        self._sub_item_log_record = {}

    def counter_log(self, leveled_names: List[str], counter_item_name: str, log_text: str):
        with self._counter_log_lock:
            # TODO: Create leveled item and +1
            pass

    def get_classified_counter(self, leveled_names: List[str]) -> dict:
        # TODO: Return all { counter_item_name: count }
        pass

    def sub_item_log(self, leveled_names: List[str], sub_item, status: str):
        with self._sub_item_log_lock:
            # TODO: Create leveled item and set status
            pass

    def get_sub_item_statistics(self, leveled_names: List[str]):
        # TODO: Return { status: subitem }
        pass









请理解为该类编写测试用例，请考虑各种情况，且准备足够的数据。注意：不要使用随机数据进行测试











python的requests和playwright使用的proxy参数格式是不同的，一个是
        {
            "http": "socks5://user:password@proxy_host:port",
            "https": "socks5://user:password@proxy_host:port"
        }
另一个是
            {
                "server": "proto://host:port",
                "username": "",  # Explicit empty string if no auth
                "password": ""   # Explicit empty string if no auth
            }

我希望对于一个给定的代理，无论是什么格式，先转换成一个中间格式，再用两个函数分别转成requests及playwright格式。这个函数需要考虑到常见的代理格式及异常和错误格式处理，可以打印错误信息，但不能抛出异常。所有注释及文本需要使用英文。









为我给这个类增加一个sub_item_log的限制功能，这个限制对所有子项生效，且限制的是子项的总数而非各个状态分类下的数量。
如果item数量超过设置的限制，则从头删除数据（删除最旧的），直到符合限制。注意考虑dict是否有序，如何保证删除的必然是先记录的。

注释和文本需要使用英文，只需要输出修改的函数代码，不要输出部分代码或无关代码。。









APPENDIX_MAX_RATE_CLASS = '__MAX_RATE_CLASS__'
APPENDIX_MAX_RATE_SCORE = '__MAX_RATE_SCORE__'

分析以下代码，为该查询增加一个threshold参数，用以筛选'APPENDIX'字段下APPENDIX_MAX_RATE_SCORE（使用该变量，不要硬编码）大于threshold的记录。
如果没有需要，不要对原有代码进行过多改动。所有注释和文本需要使用英文。








请分析该网页生成的模板，做以下更改：

1. 归档时间：article['APPENDIX'][APPENDIX_TIME_ARCHIVED]。注意字符串和常量的区别，而且这个字段和它的父字段可能不存在。
2. 最高分的分类及该类的评分：article['APPENDIX'][APPENDIX_MAX_RATE_CLASS],article['APPENDIX'][APPENDIX_MAX_RATE_SCORE]
> 可以参考下面文章页面对评分的示例代码。
3. 移除“IIS URL”
注意美观和设计，所有注释和文本使用英文。








写一个python程序，用以遍历mongodb指定记录的所有documents，找到'RATE'字段这个dict中除APPENDIX_MAX_RATE_CLASS_EXCLUDE项外，其value最大的一项。
分别将对应的key和value放入'APPENDIX'下的APPENDIX_MAX_RATE_CLASS和APPENDIX_MAX_RATE_SCORE中。









请改造下面这个函数，增加以下功能：
1. 选择器可以指定多个，按顺序以此提取内容，并组合在一起
2. 可以指定排除的选择器，通过选择器选择后，再排除子项中所有排除器指定的标签
3. 说明中详细描述选择器/排除器的格式
4. 所有文本和注释需要使用英文











用python写一个名为CrawlRecord的类，用以记录抓取的结果。功能如下：

1. 使用sqlite永久存储，启动时载入一定数量的记录作为快速查询，该数量同时也是内存缓存数据的限制（到达上限移除index最小的数据）。
2. 记录的字段包括：自动递增的index，url，结果，计数，附加信息，创建时间，更新时间
3. 提供的接口包括：
    记录url的状态
    获取url的状态（默认内存/查数据库则需要显示指定）
    指定url错误计数 +1
    获取url的错误计数（默认内存/查数据库则需要显示指定）
    清除url的错误计数
4. 状态包括（使用整数减少存储和计算量）：
    不存在 - -1

    未知 - 0
    数据库查询失败 - 1

    (抓取)错误 - 10

    (抓取)成功 - 100
    (抓取)忽略 - 110

    用户可以自行设置10及以上的值，只需要定义常量即可。10以下的值供返回结果用，不能直接写入。

构造参数如下：
    字符串数组，以其最后一项作为数据库名，前面的项依次作为目录，在创建数据库时要确保各级目录建立。
    启动时载入内存的项数，注意要载入index最大的top N。
    自动检查数据库存在性并建立。
    错误可以打印信息，但不能对外抛出异常。

注意：代码中的注释和文本需要使用英文。

请先检查根据你的经验，还有什么改进和修改建议可以提出来。







    def test_large_rw_and_reload(self):
        # TODO: 构造2000条数据，随机更新状态和错误计数

        # 释放并重新构造CrawlRecord
        self.record.close()
        self.record = None

        gc.collect()

        self.record = CrawlRecord([self.db_dir, self.db_name])

        # TODO: 检查构造后载入的是否最后1000条

        # TODO: 重复几次上述操作。
















用python实现一个基于sqlite的用户权限管理系统。请仔细思考表需要哪些字段，以及为了安全考虑需要哪些特性。就我而言，需要保证代码满足以下要求：

1. 密码不得存储明文
2. 需要至少两张表：用户账号密码表和登录操作记录表
3. 用户表至少需要以下字段：ID，用户名，密码，权限（数量未定，希望可扩展，你提供设计思路）
4. 登录操作记录表表至少需要以下字段：ID，用户，用户主机（IP），尝试的错误密码，结果
5. 考虑多线程保护
6. 采用成熟的设计方案
7. 目标是一个小型系统，可以保留扩展性，但不得过度设计
8. Log及文本需要使用英文

表设计参考如下（可自行选择删减、修改或扩展）：

CREATE TABLE user_account (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    username TEXT UNIQUE NOT NULL,
    password_hash TEXT NOT NULL,  -- bcrypt 哈希值
    is_active INTEGER DEFAULT 1,  -- 1=激活, 0=禁用
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    permission_mask (尽可能多的位域)
);


CREATE TABLE login_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER,              -- 可为空（用户不存在时）
    username TEXT NOT NULL,       -- 记录输入的用户名
    client_ip TEXT NOT NULL,
    attempted_password_hash TEXT, -- SHA256(错误密码)
    result TEXT CHECK(result IN ('SUCCESS', 'FAILURE')) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES user_account(id)
);









仔细阅读分析下面这个类，为其增加获取出所有用户信息、获取所有角色信息，获取所有权限信息的功能。

仔细阅读分析下面这个类，为其增加获取登录日志的功能。

仔细阅读分析下面这个类，为其增加删除角色的功能。





为UserManager写一个基于控制台的程序，提供易于用户操作的界面(菜单界面，非命令行)，功能包括：

1. 列出（考虑分页查看）所有用户
2. 列出（考虑分页查看）所有角色
3. 列出（考虑分页查看）所有权限

4. 增加/删除/修改用户信息
5. 为用户增加或移除角色
6. 增加/删除/修改角色信息（由代码可知，不存在的权限能自动添加，故不需要单独的权限管理功能）

7. 验证用户密码功能
8. 查看UserManager登录日志的功能
9. 其它你认为有必要的功能

请仔细思考使用什么方案实现最为便捷。












这是渲染文章列表的代码，我怎样能做到让archived_time区域的背景色根据archived_time距今的时间变化而渐变。
比如当前时间为好看的橙色（显眼但不突兀），当前时间 - 12小时为当前的浅蓝色，中间的颜色线性渐变，从而能让读者从颜色上能显著感知archived_time距当前的远近程度。








请帮我实现下面的IntelligenceCache类，用以缓存指定时间段内的情报数据以加速查询。
其中load_cache()用过IntelligenceQueryEngine查询数据库中的记录，相关的上下文如下。
注意cache中存储的是dict，即ArchivedData需要转换为dict后存储。
此外涉及到插入和检索数据时注意列表顺序为降序，即时间最接近当前的项处于列表前部。

```
class ProcessedData(BaseModel):
    UUID: str = Field(..., min_length=1)
    INFORMANT: str = Field(..., min_length=1)
    PUB_TIME: str | datetime.datetime | None = None

    TIME: list | None = Field(default_factory=list)
    LOCATION: list | None = Field(default_factory=list)
    PEOPLE: list | None = Field(default_factory=list)
    ORGANIZATION: list | None = Field(default_factory=list)
    EVENT_TITLE: str | None = Field(..., min_length=1)
    EVENT_BRIEF: str | None = Field(..., min_length=1)
    EVENT_TEXT: str | None = None

    RATE: dict | None = {}
    IMPACT: str | None = None
    TIPS: str | None = None


class ArchivedDataExtraFields(BaseModel):
    RAW_DATA: dict | None
    SUBMITTER: str | None
    APPENDIX: dict | None


class ArchivedData(ProcessedData, ArchivedDataExtraFields):
    pass


APPENDIX_TIME_ARCHIVED  = '__TIME_ARCHIVED__'


def build_intelligence_query(
        self,
        period: Optional[Tuple[datetime.datetime, datetime.datetime]] = None,
        locations: Optional[Union[str, List[str]]] = None,
        peoples: Optional[Union[str, List[str]]] = None,
        organizations: Optional[Union[str, List[str]]] = None,
        keywords: Optional[str] = None,
        threshold: Optional[float] = None  # New threshold parameter
) -> dict:
```

请根据我给出的代码框架与注释，仔细分析并实现我的需求。注意所有的的注释文本必须使用英文，如果上下文不足，请指出来。

```
class IntelligenceCache:
    def __init__(self, db_archive: MongoDBStorage, cache_period: datetime.timedelta):
        self.db_archive = db_archive
        self.cache_period = cache_period

        self.lock = threading.Lock()
        self.cache = []     # Sorted by data['APPENDIX'][APPENDIX_TIME_ARCHIVED]

    def encache(self, data: ArchivedData):
        # TODO: Insert into self.cache by data['APPENDIX'][APPENDIX_TIME_ARCHIVED] descending.
        pass

    def load_cache(self) -> bool:
        query_engine = IntelligenceQueryEngine(self.db_archive)
        query_engine.query_intelligence()

    def get_cached_data(self, filter_func: Optional[callable], map_function: Optional[callable]) -> list:
        # TODO: Just pseudocode code.
        original_data = self.cache if not filter_func else filter(filter_func, self.cache)
        mapped_data = map(map_function, original_data)
        return mapped_data

    def _check_drop_out_of_period(self):
        # TODO: while now - self.cache[-1] > self.cache_period, drop.
        pass
```






我希望使用pymongo向“UUID”字段为某值的mongodb record中的APPENDIX.__PARENT_ITEM__列表中增加一项，更新语句应该怎么构造？








我正在编写Prompt用以让AI给情报评分，但我只有大概的概念，还没做到准确且量化的描述，且评分阶梯有所缺失。请仔细思考并帮我完善评分标准。我期望的是80%的新闻评分应当在4分以下。










我会给出一个python生成内网页的模板，其中“Analysis & Evaluation”部分会展示各个维度的评分。
我希望在“Rating”后面增加一列“Manual Rating”，可以让用户通过点击和滑动星星的方式调整各维度的评分，再提供一个按钮提交评分（怎样实现比较简单和方便呢？）。
“Manual Rating”的值从'MANUAL_RATING'取，如果不存在这个字段，则使用和“Rating”相同的默认值。
注意：实现尽可能简单，尽量和现有代码耦合少（封装函数，少改动现有代码），且提交手工评分时需要附带UUID信息以标识评分对象。









我在win11上调试基于flask的python程序，发现网络服务在运行一段时间后（约2天？）失效。现象是所有本机或外部的请求全部失败，且服务端flask的routing函数断点也没进入，log也没输出，但程序的线程都正常运行，没有任何异常或crash。查看内存占用应该算是正常，这个问题重新启动服务程序即可恢复。

请帮我分析这个问题出现的原因。









帮我使用python写一个模块用以监控本服务的各种资源使用情况，提供外部获取信息接口，做好多线程保护工作。两编写一个模块将这些数据发布成一个统计网页。具体要求如下：
统计信息包括系统和需要监控的程序（可指定多个），除了传统的CPU和内存外，还需要包括句柄等各种资源占用信息，以及你认为需要展示的各种信息。
提供以下启动参数：需要监控的pid，监听端口，其它你认为有必要的参数。
可通过webapi（最好参数简化为浏览器能直接请求）增删监控对象和相关配置。
所有文本和注释都要使用英文。
主程序在启动时会启动该程序并将自己的pid传给它（怎么做？）。








我在mongodb中存储的数据记录如下所示：

```

class ProcessedData(BaseModel):
    UUID: str = Field(..., min_length=1)
    INFORMANT: str = Field(..., min_length=1)
    PUB_TIME: str | datetime.datetime | None = None

    TIME: list | None = Field(default_factory=list)
    LOCATION: list | None = Field(default_factory=list)
    PEOPLE: list | None = Field(default_factory=list)
    ORGANIZATION: list | None = Field(default_factory=list)
    EVENT_TITLE: str | None = Field(..., min_length=1)
    EVENT_BRIEF: str | None = Field(..., min_length=1)
    EVENT_TEXT: str | None = None

    RATE: dict | None = {}
    IMPACT: str | None = None
    TIPS: str | None = None


class ArchivedDataExtraFields(BaseModel):
    RAW_DATA: dict | None
    SUBMITTER: str | None
    APPENDIX: dict | None


class ArchivedData(ProcessedData, ArchivedDataExtraFields):
    pass
```

其中APPENDIX下有以下字段：

```

APPENDIX_TIME_GOT       = '__TIME_GOT__'            # Timestamp of get from collector
APPENDIX_TIME_POST      = '__TIME_POST__'           # Timestamp of post to processor
APPENDIX_TIME_DONE      = '__TIME_DONE__'           # Timestamp of retrieve from processor
APPENDIX_TIME_ARCHIVED  = '__TIME_ARCHIVED__'
APPENDIX_RETRY_COUNT    = '__RETRY_COUNT__'
APPENDIX_ARCHIVED_FLAG  = '__ARCHIVED__'
APPENDIX_MAX_RATE_CLASS = '__MAX_RATE_CLASS__'
APPENDIX_MAX_RATE_SCORE = '__MAX_RATE_SCORE__'

APPENDIX_MANUAL_RATING  = '__MANUAL_RATING__'

APPENDIX_LINK_ITEMS     = '__LINK_ITEMS__'
APPENDIX_PARENT_ITEM    = '__PARENT_ITEM__'
```

请帮我组织一个mongodb的查询，指定一个时间范围（依据APPENDIX_TIME_ARCHIVED），统计所有记录在 APPENDIX_MAX_RATE_SCORE 上的分布（评分为1 - 10），并通过网络发布，由前端绘制柱状图。
要求除前端外代码由python实现，使用Flask提供网络服务。
所有文本和注释均使用英文。







请帮我组织一个mongodb的查询，依据APPENDIX.{APPENDIX_TIME_ARCHIVED}，统计在不同时间段上的记录数量分布，并通过网络发布，由前端绘制柱状图。
时间段分为以下几档：
1. 按小时，最大滑动显示窗口为24小时
2. 按日，最大滑动显示窗口为31天
3. 按周，最大华东显示窗口为10周
4. 按月，最大滑动显示窗口为12个月
可以通过前端UI界面调整显示的时段，为了简单起见，小时按整小时为起始，日、周、月同理
要求除前端外代码由python实现，使用Flask提供网络服务。
所有文本和注释均使用英文。
先给出python后端的实现。










接下来我将给你一个python渲染的页面代码，帮我做以下改进：

界面做一定程度的美化，现在的界面太素，时间控件也不好用
脚本和HTML分离，在生成页面时再组合，使得以后可以独立调整网页风格和JS代码
使用网络资源，不要增加窗外的js及css文件。









这个页面的显示效果不错，但查询做的不够人性化。
可以尝试这样设计：指定一个开始，再通过slide bar指定一个查询时间段。
开始时间指定最好能做到：
    按小时查询时，选择的时间粒度最小为小时，范围为8 ~ 48；
    按日查询时，选择的时间粒度最小为日，范围为7 ~ 31；
    按周查询时，选择的时间粒度最小为日，范围为4 ~ 52；
    按月查询时，选择的时间粒度最小为月，范围为4 - 12；
用户选择起始时间和范围后，下方动态显示调整后的实际查询时间范围。即起始时间会被调整到每个查询周期的最初时间，周按周一对齐，月按1号对齐。
所有注释文件需要使用英文，使用网络资源，不要增加窗外的js及css文件。
如果边界清晰，只需要告知并给出需要更新的代码部分。











诸如以下flask的routing函数，我应该怎样做才能跳转到登录界面，让用户登录之后再返回到之前请求的页面呢：

```
@self.app.route('/statistics/score_distribution', methods=['GET', 'POST'])
@WebServiceAccessManager.login_required
def get_score_distribution():
```

其中login_required实现如下：

```
@staticmethod
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if 'logged_in' not in session or not session['logged_in']:
            return redirect(url_for('login'))
        return f(*args, **kwargs)
    return decorated_function
```








我们已经有以下一个脚本：

```Scripts/mongodb_exporter.py
import subprocess
import json
import argparse
from typing import Dict, Optional


def export_mongodb_data(
        uri: str,
        db: str,
        collection: str,
        output_file: str,
        query: Optional[Dict] = None,
        fields: Optional[str] = None,
        export_format: str = "json"
) -> None:
    """
    导出MongoDB数据到文件（兼容mongoimport导入格式）

    参数:
    uri: MongoDB连接字符串 (e.g. "mongodb://user:pass@localhost:27017")
    db: 数据库名称
    collection: 集合名称
    output_file: 输出文件路径
    query: 导出数据的查询条件 (e.g. {"age": {"$gt": 25}})
    fields: 指定导出字段 (e.g. "name,age,email")
    export_format: 导出格式 ("json" 或 "csv")
    """
    # 构建基础命令
    cmd = [
        "mongoexport",
        f"--uri={uri}",
        f"--db={db}",
        f"--collection={collection}",
        f"--out={output_file}"
    ]

    # 添加格式参数
    if export_format.lower() == "csv":
        cmd.append("--type=csv")
        if not fields:
            raise ValueError("导出CSV格式时必须指定--fields参数")

    # 添加查询条件
    if query:
        cmd.append(f"--query='{json.dumps(query)}'")

    # 添加字段选择
    if fields:
        cmd.append(f"--fields={fields}")

    # 执行导出命令
    try:
        result = subprocess.run(
            " ".join(cmd),
            shell=True,
            check=True,
            capture_output=True,
            text=True
        )
        print("✅ 导出成功!")
        print(f"📁 文件路径: {output_file}")
        print(f"📊 导出格式: {export_format.upper()}")
        if query:
            print(f"🔍 查询条件: {json.dumps(query)}")
    except subprocess.CalledProcessError as e:
        print(f"❌ 导出失败: {e.stderr}")
    except FileNotFoundError:
        print("❌ 未找到mongoexport工具，请安装MongoDB数据库工具")


if __name__ == "__main__":
    # 命令行参数解析
    parser = argparse.ArgumentParser(description="MongoDB数据导出工具")
    parser.add_argument("--uri", required=True, help="MongoDB连接URI")
    parser.add_argument("--db", required=True, help="数据库名称")
    parser.add_argument("--collection", required=True, help="集合名称")
    parser.add_argument("--output", required=True, help="输出文件路径")
    parser.add_argument("--query", type=json.loads, help="查询条件(JSON格式)")
    parser.add_argument("--fields", help="导出字段(逗号分隔)")
    parser.add_argument("--format", choices=["json", "csv"], default="json", help="导出格式")

    args = parser.parse_args()

    # 执行导出
    export_mongodb_data(
        uri=args.uri,
        db=args.db,
        collection=args.collection,
        output_file=args.output,
        query=args.query,
        fields=args.fields,
        export_format=args.format
    )
```

帮我实现一个前端页面和对应的基于Flask的python后端，用以调用该脚本导出数据库数据到服务器本地目录。

前端需要一定美化，提供时间段选择，点击导出后根据选择的时间段导出数据，并显示进度和结果。
可以通过三种方式选择时间段：
    通过时间控件手动任意选择间隔
    当前月 ~ 当前月-12个月的预设按钮（12个），用以快速设置时间控件
    当前周 ~ 当前周-52周的预设按钮（52个），用以快速设置时间控件

后端只需要接收时间段即可执行导出，剩余的参数参考以下代码：
    date_query = {
        "PUB_TIME": {
            "$gte": {"$date": start_date},
            "$lte": {"$date": end_date}
        }
    }

    # 导出 IntelligenceIntegrationSystem 数据库中的 intelligence_archived 集合本月记录
    export_mongodb_data(
        uri="mongodb://localhost:27017",  # 根据实际情况修改
        db="IntelligenceIntegrationSystem",
        collection="intelligence_archived",
        output_file='文件名+时间段作为文件名后缀',
        query=date_query,
        export_format="json"
    )

所有文本和注释需要使用英文。
该功能比较复杂，请一步步思考，逐步实现该功能。






Waitress

帮我写一个python脚本（跨平台），使用gunicorn启动IntelligenceHubLauncher.py，并得到它的pid。主要参数如下，不要使用配置文件而直接使用参数：
    bind = "0.0.0.0:5000"
    workers = 1     # 暂时不支持多进程，如果能开多线程最好
    worker_class = 'gevent'
    accesslog = './access.log'
    errorlog = './error.log'
启动后定时访问一个特定的服务（/maintenance/ping），如果发现得不到回复，则重启IntelligenceHubLauncher.py（限制重启间隔防止反复重启）并记录日志。
如果有没有考虑到的地方，帮我补充。











我写了一段prompt用来分析情报（新闻），在我统计评分区间时，我发现大部分评分集中在6分，而我期望的是绝大部分应当落在4分，6分及以上的重要新闻不应该这么多。从实际的阅读体验来看，我认为AI的评分过于宽松。
请帮我修改该prompt，使得评分标准更为严格，确保6分以上的情报能筛选出真正有情报价值，有影响力的情报。











我想使用python实现以下功能：不依赖任何和UI有关的库（因为是服务器），在一个线程中指定时间间隔，或定期（比如指定时间，或每周末/初，每月末/初）运行指定任务。请给出最适合的方案及代码。


使用python实现一个基于apscheduler的类，实现以下特性：
1. 分别提供独立且方便的接口以添加各种风格的scheduler（周期，定时，按周，按月，按某时间条件等等）
2. 指定是新建线程执行任务（通过一个线程wrapper）还是直接执行任务
3. 可以手动触发某任务一次，可以指定延时，0为立即触发
4. 对于周期任务，可以手动重置计时
5. 对于线程，管理其生命周期，超时结束任务线程
6. 保存执行状态，比如daily任务今天已经触发过了，即使重启程序也不会再次执行该任务
7. 所有文本注释需要使用英文










这个函数再增加两个特性，即delay_seconds和，reset_timer。

如果设置了delay_seconds，则可以将其再次加入到一个一次性任务中并延迟触发（不移除原来的调度）。
如果设置了reset_timer，则重置定时执行的定时器（对于interval_task而言）。










我期望这样设计一个request计时机制：
1. 在before_request时记录当前时间及访问路径，并生成一个uuid，作为key，记录到g里，且记录到一个dict中。
2. 在after_request，如果g中存在上文所述key，则移除，同时对于请求时间超过阈值的打印warning。
3. 提供一个函数，dump上文所述dict中，持续时间超过阈值10倍的记录到logger.warning，超过100倍的记录到logger.error。
4. 其它任何你认为有助于调试的特定。












使用python实现一个sqlite和文件的混合数据库，sqlite存储索引，文件存储内容。
提供二进制及文本内容的存取接口，属性包括：
1. 分类 - 对应文件保存的文件夹名，不能有文件名目录名的非法字符（both win and linux），默认为“default”
2. 时间 - 可选，如果没指定则取当前时间
3. 名称 - 如果没指定名称，则使用uuid4作为其名称。
模块初始化时指定根目录名，文件存储路径以“分类”为子目录，以“名称”+“时间”为文件名，以txt或bin为后缀。
将文件信息和索引（记得使用基于根目录的相对路径）保存到数据库中时需一个递增index字段，文件存储成功后返回该index。

可以通过index、名称（模糊）、时间（范围）来获取文件列表，可以指定返回相对路径还是绝对路径。
docstring需要详细说明该类的设计意图和usage，以及各个接口的用途。所有文本注释使用英文。








我有一段生成文章列表的代码，其中有一个区域显示消息来源，是一个url：

```<span class="article-source">Source: {informant_html}</span>```

我希望建立一个列表，包括以下信息：世界主要媒体的域名，所属国家地区，对应的国家地区的国旗（emoji），在中国是否可直接访问。
从上面所述的url中使用亮黄色高亮其顶级域名部分，并且在URL前面增加国旗及中国是否可直接访问的符号标识（请自行选择适合的emoji）。
以上代码考虑使用JS实现，以便今后将该页面转换为完全前端的页面。
emoji图标和url显示要协调，美观，注意url有过长从而折行的可能性。
代码按照之前实现同样地将css和js分离。

这个列表尽可能包含多的主流媒体，包括并不限于以下媒体，且日后可供我手动扩展：

"""
	华尔街日报
	Investing.com
	纽约时报
	美国之音
	加拿大国际广播电台
	BBC
	法广
	德国之声
	澳大利亚广播公司
	半岛电视台
	俄罗斯卫星通讯社
	NHK日本国际传媒

	联合早报
	朝鲜日报
	中央日报
	共同网
	日经中文网
	中央社
	界面新闻
	澎湃新闻
	南方周末
	报刊
	人民日报
	光明日报
	经济日报
	解放军报
	求是
	新华每日电讯
	新京报
"""










写一个python程序，使用selenium操作chrome浏览器，打开一个AI对话网页，模拟输入并发送，等待输出并提取回复内容。

选择器参考如下：

GEMINI_SELECTORS = {
    "input_box": '[aria-label="在此处输入提示"]',
    "send_button": '[aria-label="发送"]',
    "indicator": '[aria-label="发送"][disabled]',
    "response_content": ".markdown.markdown-main-panel:last-child"
}

给出代码，注释和详细的部署方法。










我想使用实现一个资源使用情况和限制的管理功能，具体来说：

1. 配置某资源的限制，包括但不限于：
    total token
    input token
    output token
    每分钟请求数 (RPM)
    每分钟 token 数（输入）(TPM)
    每日请求数 (RPD)
    其它资源的常用限制
    自定义的限制
    ......
2. 提供接口方面更新已使用限制
3. 保存和加载使用情况
4. 根据限制的周期重置使用情况

是否有现成的，较为轻量的python库能实现上面的需求？








使用python实现一个ResourceUsageManager（或更合适的名字），实现对有限制资源的管理：

1. 每个资源有唯一的名字（外部传入，无则内部生成）。
2. 资源使用量抽象为数量 + 重置时间
    2.1 对于纯使用量的资源，时重置时间不存在
    2.2 对于按时间段周期重置的资源，使用滑动窗口统计使用情况
    2.3 对于按固定时刻重置的资源，使用cron的方式设计重置时刻，cron接口可以拆分多个以便于设置yearly, monthly, weekly, daily, hourly重置的资源（cron的通用接口不友好）。
3. 一个资源可能有多重限制，即CompositeLimit。所以限制要和资源分离，一个资源可以包含一个到多个限制，检查可用性时应当返回其中使用量最大者（即，从严判定）。
4. 资源需要一个“单位”的字符串，仅用来对用户友好显示。程序提供常用的单位，比如RPM、TPM、RPD、Token、次数、流量单位等等。
5. 提供方便的记录（提交使用量）的接口
6. 可以设置2个阈值，并提供可用性检查接口，使用量超过第一个阈值的资源不推荐，超过第二个阈值为不可用。
7. 资源需要分类（group），提供指定group名字让程序推荐一个可用的（且用量尽可能大的，避免每个都用一点的情况）接口。
8. 提供手动重置接口
9. 提供手动设置/校准接口
10. 提供统计接口（对用户和前端友好）
11. 资源管理可以存储额外数据，用户可以存储和提取真正的资源数据。
12. 资源配置可以导入和导出成json
13. 使用sqlite存储使用情况，启动时载入并对周期或固定时刻重置的资源进行检查和重置，输出初始化情况。
14. docstring要详细
15. 所有文本注释使用英文

有进一步的改进意见请提出。









根据以下信息，理解、推论并完成代码，文本及注释使用英文：
```
import logging

from Tools.OpenAIClient import OpenAICompatibleAPI
from Tools.AiServiceBalanceQuery import get_siliconflow_balance


logger = logging.getLogger(__name__)


class SiliconFlowServiceRotator:
    # 用来检查余额并自动更换key
    # 启动时载入key，当判定使用完一个key（balance < self.threshold）后，将其从self.valid_keys移除，并写入'.json'文件。
    # 更换key前先检查key的余额及有效性
    # 不要因为网络问题而错误地丢弃key
    # 为了让系统正常运作，启动时先给一个key再说
    # 如果check_all_balance为True，则在启动并检查完当前key后，获取所有key的余额
    # 保存json时需要通过临时文件
    def __init__(self,
                 ai_client: OpenAICompatibleAPI,
                 keys_file: str,
                 threshold: float = 0.1,
                 check_all_balance: bool = True):
        self.ai_client = ai_client
        self.keys_file = keys_file
        self.threshold = threshold

        self.keys_data = {}
        self.current_key = ''

    def load_keys(self):
        key_record = 'key_record.json'
        # TODO: 载入'key_record.json'，并构建内存中的key表，至少包括以下字段：余额、最后一次使用，状态（有效，无效，错误，默认：未知）
        #       同时也从self.keys_file载入所有key，并跟自己的数据库比对，加入数据库中不存在的部分
        #       keys_file的格式非常简单，每一行为一个key，记得去除首尾空格

    def check_update_key(self):
        balance = self._fetch_balance(self.current_key)

    def run_forever(self, thread_quit_flag):
        # TODO: 在这个函数中调度，包括获取所有余额信息，检查当前key余额，保存json文件，等等
        #       根据余额的趋势估计下一次检查的时间
        pass

    def _fetch_balance(self, key: str) -> float:
        if key:
            result = get_siliconflow_balance(key)
            if 'total_balance_usd' in result:
                return result['total_balance_usd']
        return -1

    def _change_api_key(self, api_key):
        self.ai_client.set_api_token(api_key)

```











请分析并理解以下代码的设计并做以下改动：

1. 初始化载入文件逻辑不变，但如果一个key的状态已经被判定为不可用，则后续所有操作都不用于该key，该key在json数据库中仅做记录。
2. 在run_forever()中选择初始key。逻辑如下：
    + 按顺序选择第一个可用的key
    + 检查其状态，如果余额小于阈值，则更新数据并换下一个，直到选择到一个可用的key。
    + 选择可用的key后一并保存刚才更新的内容到json数据库
3. 如果指定全部更新，则对可用的key做一次全部更新（不可用的key不要更新）
4. 检查当前key是否达到不可用阈值及换成新key的代码检查确认无误，废弃的key要标识为不可用，后续该key再也不会参与任何操作。

注意：
    余额可能为负值
    文本注释使用英文
    不要丢失已实现功能




















