作为一个专业的python程序员，接下来你需要使用简洁及优雅的代码实现我的需求，所有函数必须有标准的英文注释，并且考虑错误处理及可测试性。理解请回复“OK”。


我希望实现一个类，它能够根据rss订阅列表，对其中的每一个rss feed，下载其中所有的文章内容，包括：
1. 文章标题
2. 发布时间
3. 文章链接
4. 文章内容
其中rss feed列表会通过读取json文件输入，json格式如下：
```
{
  "theme": "科技",
  "focus": "AI",
  "prompt": "",
  "feeds": {
    "青柠学术": "https://qnscholar.github.io//feed.xml",
    "MOOC中国": "https://www.cmooc.com/feed",
    "知乎日报": "https://feeds.feedburner.com/zhihu-daily",
    "知乎每日精选": "https://www.zhihu.com/rss"
  }
}
```
请将读取json文件、遍历feeds、对每个feed下载、对feed中其中一篇文章的链接下载各写在不同函数里。

对于上面的需求有不清楚之处清进一步询问，如果有更多的建议也请提出。




我希望将正文转换为markdown格式以减少复杂度。




在这个函数里面，调用另一个函数，用来将不同feed的文章分文件夹存储为<标题>.md文件。注意需要保证目录存在，且<标题>作为文件名有效，否则将其中非法字符进行清洗。
不要在最后一起写入，而是拉取一个写一个，这是为了防止中途出错。另外请考虑这么一种机制：成功拉取文章后记录一个“URL” - "文件"的持久性存储表，在每次拉取时，不要拉取已经拉取过的文章。





帮我写一个FeedsValidation的python脚本，提供本地API，WebAPI，命令行和界面四种界面。
本地API行为如下：
    传入一个URL，同步验证其是否能得到一个合法的RSS XML数据。
    传入一个Feeds字典，异步验证其中每一个URL能得到一个合法的RSS XML数据。
    维护一个 {Feed URL：状态} 字典，用户可以调用对应接口查询Feed的状态（未知，有效，无效，Busy等等）。
    提供另一个接口，清除Feed状态字典。
WebAPI则支持提交一个Feeds字典，同样更新到{Feed URL：状态} 字典，通过另一个wei api查询状态。
    同时给出调用该服务的测试代码。
命令行则接受一个url，同步验证其是否能得到一个合法的RSS XML数据，返回结果并退出。
界面则使用pyqt5，左侧输入单个链接或json，右侧则以列表显示每个feed链接的状态，feed状态的更新必须是异步的。
    用户可以勾选列表项，列表下方根据勾选项目动态生成json。程序提供“全选/全不选/仅选有效Feed/清除无效Feed”功能
如果不带命令行参数，默认启动界面，如果参数为url，则使用命令行。你来指定一个参数让可以启动web api服务。
将上面的功能实现在同一个py文件里，注意每个功能的独立性，比如pyqt5不存在不影响命令行的使用。

接受的JSON的格式如下：
"""
{
  "other keys just ignore": "........",
  "feeds": {
    "feed name 1": "feed url 1",
    "feed name 2": "feed url 2",
  }
}
"""

其中：
    列表要分列显示，列包括：Feed Name, Feed URL, Status
    动态生成的json项为  Feed Name: Feed URL，而非状态。





这个函数会遇到403的问题，我想，既然是验证RSS，那么正确的方法应该是利用标准的RSS请求来获取RSS资源，而非直接request爬取。
请帮我改造这段代码，在保持现有逻辑的前提下，使用标准的RSS请求来验证feed是否有效。






我使用feedparser库拉取并分析rss feed，但我发现该库反爬能力太弱，以致于对于很多feed根本拉取不到。
所以我希望通过无头浏览器拉取内容，再通过feedparser分析feed内容。
请帮我实现一个基于无头浏览器的强大网页内容获取程序，要求支持包括socks5在内的代理。





帮我实现一个parse_feed函数，参数是从rss feed返回的内容（content），你需要使用合适的库和工具对其进行解析。
该程序需要应对各种复杂和错误的情况，并且对每rss中的每一条数据，重新组织和返回关键字段的内容（请想一下rss的关键信息有哪些）。
注意不要出现语法错误和示实现的引用，不要简单的拼接代码，而是分析其功能和适用性，将其融合到自己的代码中。





我先给出requirements.txt中的内容，再给出所有python文件中import的内容，
帮我检查requirements.txt中是否有多余的依赖，或者缺失的依赖，最终输出完整且移除多余引用的requirements.txt内容。

requirements.txt :
"""
"""

Imports :
"""
"""







作为一个专业的python程序员，接下来你需要使用简洁及优雅的代码实现我的需求，所有函数必须有标准的英文注释，并且考虑错误处理及可测试性。

我希望通过url抓取一个网页，其中url来自RSS feed的XML文章描述。
帮我实现一个能应对大多数反爬情况的网页抓取模块，需要支持包括socks5在内的代理，但不需要考虑过于复杂的挑战的情况。
即尽可能地模拟浏览器浏览和渲染过程，需要考虑抓取的成功率，而不需要过多考虑抓取的效率。
需要考虑抓取函数可重入或抓取模块多实例化，以便多线程抓取。
请审视编写的代码，确定没有任何的引用未被实现。
在其它模块中使用了playwright，如果合适，你也可以考虑使用它。








我使用sync_playwright和requests实现不同模块来抓取网页，但它们的代理配置似乎不一样。

sync_playwright的格式是：

    proxy_config = {
        "server": "socks5://127.0.0.1:10808",
        "username": "",
        "password": ""
    }


而requests的格式是：


    proxies = {
        "http": "socks5://user:password@proxy_host:port",
        "https": "socks5://user:password@proxy_host:port"
    }


我希望它们能使用同样的格式，或者通过适配函数将格式转成一致。
注意要考虑到代理不需要认证，即username和password不存在的情况，一个函数要能同时处理有认证的情况和无认证的情况。
各个函数需要有标准英文注释，包括描述输入格式，输出格式。





针对 parse_proxy，to_playwright，to_requests，考虑各种情况，包括非法输入情况，为它们各自编写测试函数。
不需要使用任何测试框架，只需要使用assert进行断言即可。






在我的情报采集系统中，我想写一个prompt让AI给网络信息进行评分，分制为0 - 10分。
很显然，广告和完全无效的内容评分为0分。反之，诸如911和特朗普遇袭这类国际重大事件评为10分。
但对于中间具体的评分标准，我还没有思路。能不能帮助我制定关于情报价值评分标准的描述？如果有现成的成熟通用标准，那么最好。







帮我实现一个名为IntelligenceHub的类，它采用多线程实现以下功能：
1. 信息收集模块：提供WEB API，接受POST请求，参数是DICT，除UUID必须有之外，其余字段皆不限制，将收集信息放入待处理队列中。
    同时提供调用该WEB API的本地API（proxy）
2. 信息处理模块：从队列中获取待处理数据，通过POST请求调用WEB接口。该接口参数同样是DICT，除UUID外其余字段皆不限制。
   它将采用UUID作为消息的唯一标识，当消息异步处理结束后，该UUID将用来匹配之前的请求。
   如果一个数据在一定时间内未得到处理，将作为超时处理，将其放回待处理列表中，根据重试次数决定它的优先度。
3. 信息处理反馈模块
    提供WEB API，接受处理之后的消息，使用UUID匹配源消息。将源消息和处理后的消息一起放入队列中给后处理和归档模块。
    同时提供调用该WEB API的本地API（proxy）。
4. 后处理和归档模块：
    归档：将处理后的结果放入mongodb中，并建立指向它的向量数据库索引。
    后处理暂时未定，有可能采用插件的方式实现。
以上只是一个大概的框架，细节部分请加上TODO标记，由后续逐步实现。
一定一定要注意并发场景及效率，不要出现资源冲突的情况。



我实现了一个名为IntelligenceHub的类，它采用多线程实现以下功能：
1. 信息收集模块：提供WEB API，接受POST请求，参数是DICT，除UUID必须有之外，其余字段皆不限制，将收集信息放入待处理队列中。
    同时提供调用该WEB API的本地API（post_collected_intelligence）
2. 信息处理模块：从队列中获取待处理数据，通过POST请求调用WEB接口。该接口参数同样是DICT，除UUID外其余字段皆不限制。
   它将采用UUID作为消息的唯一标识，当消息异步处理结束后，该UUID将用来匹配之前的请求。
   如果一个数据在一定时间内未得到处理，将作为超时处理，将其放回待处理列表中，根据重试次数决定它的优先度。
3. 信息处理反馈模块
    提供WEB API，接受处理之后的消息，使用UUID匹配源消息。将源消息和处理后的消息一起放入队列中给后处理和归档模块。
    同时提供调用该WEB API的本地API（post_processed_intelligence）。
4. 后处理和归档模块：
    归档：将处理后的结果放入mongodb中，并建立指向它的向量数据库索引。
    后处理暂时未定，有可能采用插件的方式实现。

请一步步地分析该程序，帮我实现和改进你认为可以进一步实现或改进的地方。




请分析IntelligenceHub的功能和行为，并为IntelligenceHub编写包括Collector模拟, IntelligenceProcessor模拟在内的测试辅助模块。





基于它们编写测试用例，先测试整个处理流程，然后分条列出测试各种情况以及测试性能的测试用例简要描述。先不要输出测试代码。
先不要输出testcase的实现。






processor_thread是一个IO线程，因为等待超时所以会成为性能瓶颈。我希望按照CPU数除以2创建一个线程池，由该线程池来进行数据处理。
注意不要频繁创建和退出线程，而是通过信号里让线程等待数据，并且不影响检查退出标记。相关代码如下：








理解下面一段代码，增加一个类，功能如下：
    提供一个接口，参数与load_json_config()一样，名字请根据功能自行推测。
    调用该函数在读取json配置后，会创建一个线程以polling_interval周期循环执行processor.process_all_feeds()。
    要注意使用线程退出标志让线程有优雅退出的机会，以文件名为索引可以找到并管理线程。
    如果发现参数中的文件名已存在线程列表中，如果指定force，则关闭旧线程开启新线程，否则返回错误并忽略。
    该类同时还要提供统计信息获取接口，并注意多线程资源访问问题。







写一个函数，接受两个参数：key属性表，待检查的dict。
key属性表类似下面的例子：

COLLECTOR_DATA_FIELDS = {
    'UUID': 'M',
    'source': 'O',
}

基中M代表MUST，O代表OPTIONAL。函数返回 (MUST字段的缺失数，OPTIONAL字段的存在数，不在属性表中的字段数)






python有没有可能实现这样一种机制：
当我向特定文件夹放入一个python文件时，这个新加入文件会被服务识别并从指定的函数启动。
当我从这个文件夹中移除一个文件时，服务会在该文件对应的任务执行完毕后移除该任务（移除文件对程序运行本身没有影响）。
请仔细分析这个需求，分析其中的问题及实现可能性，给出上面提到的“服务”程序的代码。







python有没有这样的机制：开辟一个有P个线程的线程池，执行T个任务（T可能大于P）。
里面的任务不会发生抢占







下面这段代码，由于一个sql连接无法在线程间共享，因此我希望将整个save_content做成异步的。
即在该单例初始化时创建一个线程，调用save_content时仅把数据放入队列中，并通知线程做后续操作。
该类需提供优雅退出线程的方法，并且注意线程安全问题。



使用crawl4ai实现以下接口，要求能对抗大部分常用的反爬手段，并且提供分析抓取失败的诊断手段。


class ProxyConfig(TypedDict):
    http: str
    https: str


class ScraperResult(TypedDict):
    content: str
    errors: List[str]


def fetch_content(
    url: str,
    timeout_ms: int,
    proxy: Optional[ProxyConfig] = None,
    **kwargs
) -> ScraperResult:




写一段python程序，使用openai的接口调用硅基流动的API接口（是不是理论上可以支持所有openai like接口？）。
token可以在构造时从外界传入，如果没有，则去环境变量中获取。
可以选择采用同步或异步的方式获取结果。
所有函数都需要标准注释，且关键地方也需要注释说明意图和数据格式，所有注释必须为英文。













基于我们的对话上下文，请分析并理解我的需求，基于上面的AI接口，编写一个函数实现下面描述的功能：

输入：主prompt，结构化的数据（json，其中主要内容是content字段，其它比如title字段等等为可选），对话上下文（可选）
输出：按prompt输出的json格式。
函数功能：将输入的prompt和结构化数据传递给AI，等待输出。输出应当为json格式，将其转为dict输出。

主prompt示例如下：









程序中已通过以下代码初始化好了mongodb client：

"""
def _setup_mongo_db(self) -> bool:
    """初始化MongoDB连接并创建必要索引"""
    try:
        # 连接配置
        self.mongo_client = pymongo.MongoClient(
            self.mongo_db_uri,
            maxPoolSize=100,
            serverSelectionTimeoutMS=5000
        )

        # 验证连接
        self.mongo_client.admin.command('ping')
        self.db = self.mongo_client["intelligence_db"]
        self.archive_col = self.db["processed_data"]

        # 定义索引规范（名称、字段、选项）
        index_specs = [
            {
                "name": "created_at_idx",
                "keys": [("metadata.created_at", pymongo.ASCENDING)],
                "description": "基于创建时间的时间范围查询加速"
            },
            {
                "name": "vector_dim_idx",
                "keys": [("vector.dim", pymongo.ASCENDING)],
                "options": {
                    "partialFilterExpression": {"vector": {"$exists": True}},
                    "background": True  # 后台创建避免阻塞
                },
                "description": "向量维度查询优化（仅当存在vector字段时建立）"
            },
            {
                "name": "content_text_idx",
                "keys": [("raw_data.value", "text")],
                "options": {
                    "weights": {"raw_data.value": 5},
                    "default_language": "english",
                    "language_override": "language"
                },
                "description": "全文检索索引（字段权重优化）"
            }
        ]

        # 检查并创建索引
        existing_indexes = {idx["name"]: idx for idx in self.archive_col.list_indexes()}

        for spec in index_specs:
            index_name = spec["name"]
            index_model = pymongo.IndexModel(spec["keys"], **spec.get("options", {}))

            # 索引存在性检查
            if index_name in existing_indexes:
                existing = existing_indexes[index_name]
                # 检查索引定义是否一致
                if (existing["key"] == index_model.document['key'] and
                        existing.get("partialFilterExpression") == spec.get("options", {}).get(
                            "partialFilterExpression")):
                    logger.info(f"索引 {index_name} 已存在，跳过创建")
                    continue

                # 删除不一致的旧索引
                logger.warning(f"检测到不一致索引 {index_name}，重新创建...")
                self.archive_col.drop_index(index_name)

            # 创建新索引
            logger.info(f"创建索引 {index_name}：{spec['description']}")
            self.archive_col.create_indexes([index_model])

        return True
    except pymongo.errors.OperationFailure as e:
        logger.critical(f"MongoDB索引操作失败: {str(e)}")
        return False
    except Exception as e:
        logger.critical(f"MongoDB连接失败: {str(e)}")
        return False
"""


请写一个函数将dict写入该数据库。dict示例如下（省略部分数据）：

"""
C:\D\Code\git\IntelligenceIntegrationSystem\.venv\Scripts\python.exe -X pycache_prefix=C:\Users\yq1061170\AppData\Local\JetBrains\PyCharmCE2024.2\cpython-cache "C:/Program Files/JetBrains/PyCharm Community Edition 2024.2.3/plugins/python-ce/helpers/pydev/pydevd.py" --multiprocess --qt-support=auto --client 127.0.0.1 --port 50471 --file C:\D\Code\git\IntelligenceIntegrationSystem\Tools\IntelligenceAnalyzerProxy.py
Connected to pydev debugger (build 242.23339.19)
AI response spends 94.8275637626648 s
{
  "UUID": "55eebb04-1400-4b33-9800-7fdb43ad4689",
  "TIME": "2024-07-19",
  "LOCATION": "",
  "PEOPLE": [],
  "ORGANIZATION": [
    "Netflix",
    "Paramount",
    "Disney",
    "Amazon",
    "Corus",
    "Global TV",
    "",
    "Motion Picture Association-Canada"
  ],
  "EVENT_BRIEF": "",
  "EVENT_TEXT": ",
  "RATE": {
    "战略相关性": 6,
    "国际关联度": 4,
    "金融影响力": 6,
    "政策关联度": 8,
    "科技前瞻性": 2,
    "投资价值": 6,
    "内容准确率": 8
  },
  "IMPACT": "",
  "TIPS": "流媒体与传统媒体就加拿大内容资金义务产生政策博弈"
}

Process finished with exit code 0

"""




作为一个资深专业的python程序员，帮我实现一个mongodb存储类，要求如下：
1. 存储通用的dict，而不要去管dict中的键值。
2. 提供基本的查询接口。
3. 良好的可配置性，如服务地址，数据库名，登录认证等等，参数配置需配上详细说明。
4. 多线程访问安全，保证资源不泄露。
5. 适度通用的优化，以增加访问速度，优化需要配上详细说明。
6.注释必须标准且为英文，所有文字都需要是标准专业的英文。




使用并改造以下的类，实现以下功能：
1. 增加一个update的方法，指定一个或多个字段以及值（dict），并指定需要更新或增加的字段（dict），更新对应记录。
2. 增加一个查询接口，指定，指定一个或多个字段以及值（dict），返回记录的dict列表（转成dict，而非原始的查询记录）











使用python，基于faiss实现一个向量数据库类，提供以下功能：

1. 能指定存储路径或数据库名
2. 指定一个id(str)和一段文本，为文本生成相应的向量并与id关联存储
3. 支持查询功能，提供一段文本和topN，返回匹配的id list。

要求使用合适的尽可能简单和轻量的实现，不要使用过于重型和收费的库。









帮我检查这段代码，一步步分析代码的功能和作者的用意，从以下角度思考：
1. 代码是否有问题或漏洞
2. 代码是否健壮
3. 代码是否有可以改进的地方
4. 考虑多线程的问题
综合以上分析，最终给出你的改进建议。














