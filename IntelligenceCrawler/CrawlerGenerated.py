# CrawlerGenerated.py - This code is generated by CrawlerPlayground

from IntelligenceCrawler.Fetcher import *
from IntelligenceCrawler.Extractor import *
from IntelligenceCrawler.Discoverer import *
from IntelligenceCrawler.CrawlPipeline import CrawlPipeline

log_cb = print

def run_pipeline():
    # === 1. Initialize Components ===
    d_fetcher = PlaywrightFetcher(log_callback=log_cb, proxy=None, timeout_s=10, stealth=True, pause_browser=False, render_page=False)
    e_fetcher = PlaywrightFetcher(log_callback=log_cb, proxy=None, timeout_s=20, stealth=True, pause_browser=False, render_page=True)
    discoverer = RSSDiscoverer(fetcher=d_fetcher, verbose=True)
    extractor = TrafilaturaExtractor(verbose=True)
    
    # === 2. Define Parameters ===
    entry_point_urls = ['http://feeds.bbci.co.uk/news/rss.xml']
    days_ago = 7
    end_date = datetime.datetime.now()
    start_date = end_date - datetime.timedelta(days=days_ago)
    extractor_kwargs = {}

    # === 3. Build pipeline ===
    pipeline = CrawlPipeline(
        d_fetcher=d_fetcher,
        discoverer=discoverer,
        e_fetcher=e_fetcher,
        extractor=extractor,
        log_callback=log_cb
    )

    # Step 1: Discover all channels
    pipeline.discover_channels(entry_point_urls, start_date, end_date)

    # Step 2: Discover and fetch articles (populates pipeline.contents)
    pipeline.discover_articles(channel_filter=lambda url: True)

    # Step 3: Extract content and run handlers
    pipeline.extract_articles(
        article_filter=lambda url: True,
        content_handler=lambda url, result: print(f"URL: {url}\nText:\n {result}"),
        exception_handler=lambda url, exception: None,
        **extractor_kwargs
    )


if __name__ == "__main__":
    try:
        run_pipeline()
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())
