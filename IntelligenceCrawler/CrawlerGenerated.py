# CrawlerGenerated.py - This code is generated by CrawlerPlayground

import json
import time
import datetime
import traceback
from IntelligenceCrawler.Fetcher import *
from IntelligenceCrawler.Extractor import *
from IntelligenceCrawler.Discoverer import *
from IntelligenceCrawler.CrawlPipeline import CrawlPipeline

log_cb = print

def run_pipeline():
    # === 1. Initialize Components ===
    d_fetcher = PlaywrightFetcher(log_callback=log_cb, proxy=None, timeout_s=10, stealth=True, pause_browser=False, render_page=False)
    e_fetcher = PlaywrightFetcher(log_callback=log_cb, proxy=None, timeout_s=20, stealth=True, pause_browser=False, render_page=True)
    discoverer = RSSDiscoverer(fetcher=d_fetcher, verbose=True)
    extractor = TrafilaturaExtractor(verbose=True)
    
    # === 2. Define Pipeline Parameters ===
    entry_point_urls = ['http://feeds.bbci.co.uk/news/rss.xml']
    start_date = None
    end_date = None
    extractor_kwargs = {}

    pipeline = CrawlPipeline(
        d_fetcher=d_fetcher,
        discoverer=discoverer,
        e_fetcher=e_fetcher,
        extractor=extractor,
        log_callback=log_cb
    )

    # Step 1: Discover all channels
    pipeline.discover_channels(entry_point_urls, start_date, end_date)

    # Step 2: Discover and fetch articles (populates pipeline.contents)
    pipeline.discover_articles(channel_filter=None)

    # Step 3: Extract content and run handlers
    pipeline.extract_articles(
        content_handler=log_cb,
        error_handler=log_cb,
        **extractor_kwargs
    )


if __name__ == "__main__":
    try:
        run_pipeline()
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())

