# CrawlerGenerated.py - This code is generated by CrawlerPlayground

import json
import time
import datetime
import traceback
from IntelligenceCrawler.Fetcher import *
from IntelligenceCrawler.Extractor import *
from IntelligenceCrawler.Discoverer import *
from IntelligenceCrawler.CrawlPipeline import CrawlPipeline

log_cb = print

# === 1. Initialize Components ===
d_fetcher = PlaywrightFetcher(log_callback=log_cb, proxy=None, timeout_s=10, stealth=True, pause_browser=False, render_page=False)
e_fetcher = PlaywrightFetcher(log_callback=log_cb, proxy=None, timeout_s=20, stealth=True, pause_browser=False, render_page=True)
discoverer = RSSDiscoverer(fetcher=d_fetcher, verbose=True)
extractor = Newspaper3kExtractor(verbose=True)

# === 2. Define Pipeline Parameters ===
entry_point_urls = ['http://feeds.bbci.co.uk/news/rss.xml']
days_ago = 7
end_date = datetime.datetime.now()
start_date = end_date - datetime.timedelta(days=days_ago)
extractor_kwargs = {}

def run_pipeline():
    pipeline = CrawlPipeline(
        d_fetcher=d_fetcher,
        discoverer=discoverer,
        e_fetcher=e_fetcher,
        extractor=extractor,
        log_callback=log_cb
    )

    # Step 1: Discover all channels
    pipeline.discover_channels(entry_point_urls, start_date, end_date)

    # Step 2: Discover and fetch articles (populates pipeline.contents)
    pipeline.discover_articles(channel_filter=None)

    # Step 3: Extract content and run handlers
    pipeline.extract_articles(
        content_handler=log_cb,
        error_handler=log_cb,
        **extractor_kwargs
    )


if __name__ == "__main__":
    try:
        run_pipeline()
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())

