概念与设计
====

在LoRA训练结束后，我们需要加载该LoRA，使用测试集中的原始数据进行推理。同时记录模型输出和测试集中对应数据的期望输出，通过人工对比进行评判，或进行AI测评。

其中，读取测试集 -> 加载LoRA -> 进行推理 -> 记录结果的整个过程由 [validation1_batch_inference.py](validation1_batch_inference.py) 实现。

生成结果（如：result_ckpt100.jsonl）后，使用 [validation2_review_app.py](validation2_review_app.py) 加载该结果，通过网页进行人工对比和评判。

而 [validation3_auto_eval.py](validation3_auto_eval.py) 则是将结果交由AI评判（未测试）。


操作与使用
====

+ 将 [validation1_batch_inference.py](validation1_batch_inference.py) 复制到你的训练机上，在前文提到的 iis_finetune 环境中运行以下命令：

    ```bash
    # 指定使用 GPU 0
    # 视具体情况也可以是CUDA_DEVICE
    export HIP_VISIBLE_DEVICES=0
    
    # 注意模型和测试集的路径
    python validation1_batch_inference.py \
        --adapter ./saves/qwen2.5-7b-intelligence/lora/sft_ddp_fp32/checkpoint-100 \
        --data data/alpaca_test.json \
        --output result_ckpt100.jsonl
    ```

+ 将生成的结果（上面的例子为result_ckpt100.jsonl）复制到 [validation2_review_app.py](validation2_review_app.py) 相同目录下，运行以下命令：

    ```bash
    # 不需要在iis_finetune环境下
    # 但需要安装streamlit
    streamlit run validation2_review_app.py
    ```
    
    在自动打开的浏览器中可以浏览实际结果和预期的对比。

+ AI自动测评（略）
