# CrawlerConfig.py - This configuration is generated by CrawlerPlayground

# === Crawler Configuration ===
CRAWLER_CONFIG = {
    # === Components init parameters ===
    
    'd_fetcher_name': 'RequestsFetcher',
    'd_fetcher_init_param': {'log_callback': print, 'proxy': None, 'timeout_s': 10},

    'e_fetcher_name': 'RequestsFetcher',
    'e_fetcher_init_param': {'log_callback': print, 'proxy': None, 'timeout_s': 20},

    'discoverer_name': 'ListPageDiscoverer',
    'discoverer_init_param': {'verbose': True, 'manual_specified_signature': None, 'scope_selector': None},

    'extractor_name': 'Trafilatura',
    'extractor_init_param': {'verbose': True},

    # ======== Crawl parameters ========
    
    'entry_points': {'即时新闻': 'https://www.news.cn/world/jsxw/index.html', '新华财经': 'https://www.news.cn/fortune/yx/index.html', '中央文件': 'https://www.news.cn/politics/zywj/index.htm', '新华人事': 'https://www.news.cn/politics/xhrs/index.html', '廉政资讯': 'https://www.news.cn/legal/ffu/lzzx/index.html'},
    'period_filter': (None, None),
    'channel_filter': None,
    'd_fetcher_kwargs': {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 10, 'scroll_pages': 0},
    'e_fetcher_kwargs': {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 20, 'scroll_pages': 0},
    'extractor_kwargs': {},

    # === Handler functions (to be implemented by user) ===
    
    'article_filter': None,
    'content_handler': None,
    'exception_handler': None,
}


# ------ Usage example and demo. Can be deleted. ------

if __name__ == "__main__":
    import traceback
    try:
        from IntelligenceCrawler.CrawlPipeline import run_pipeline, save_article_to_disk
        CRAWLER_CONFIG['content_handler'] = save_article_to_disk
        run_pipeline('default', CRAWLER_CONFIG)
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())
