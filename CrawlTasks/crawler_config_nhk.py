# CrawlerConfig.py - This configuration is generated by CrawlerPlayground

# === Crawler Configuration ===
CRAWLER_CONFIG = {
    # === Components init parameters ===
    
    'd_fetcher_name': 'RequestsFetcher',
    'd_fetcher_init_param': {'log_callback': print, 'proxy': None, 'timeout_s': 30},

    'e_fetcher_name': 'PlaywrightFetcher',
    'e_fetcher_init_param': {'log_callback': print, 'proxy': None, 'timeout_s': 30, 'stealth': True, 'pause_browser': False, 'render_page': True},

    'discoverer_name': 'RSSDiscoverer',
    'discoverer_init_param': {'verbose': True},

    'extractor_name': 'TrafilaturaExtractor',
    'extractor_init_param': {'verbose': True},

    # ======== Crawl parameters ========

    'entry_points': ['https://www.nhk.or.jp/rss/news/cat4.xml', 'https://www.nhk.or.jp/rss/news/cat1.xml', 'https://www.nhk.or.jp/rss/news/cat6.xml', 'https://www.nhk.or.jp/rss/news/cat5.xml'],
    'period_filter': (None, None),
    'channel_filter': {'channel_list_filter': []},
    'd_fetcher_kwargs': {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 20, 'scroll_pages': 0},
    'e_fetcher_kwargs': {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 20, 'scroll_pages': 0},
    'extractor_kwargs': {},

    # === Handler functions (to be implemented by user) ===
    
    'article_filter': None,
    'content_handler': None,
    'exception_handler': None,
}


# ------ Usage example and demo. Can be deleted. ------

if __name__ == "__main__":
    import traceback
    try:
        from IntelligenceCrawler.CrawlPipeline import run_pipeline, save_article_to_disk
        CRAWLER_CONFIG['content_handler'] = save_article_to_disk
        run_pipeline(CRAWLER_CONFIG)
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())
