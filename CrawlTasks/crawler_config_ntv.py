# CrawlerConfig.py - This configuration is generated by CrawlerPlayground

# === Crawler Configuration ===
CRAWLER_CONFIG = {
    # === Components init parameters ===
    
    'd_fetcher_name': 'RequestsFetcher',
    'd_fetcher_init_param': {'log_callback': print, 'proxy': None, 'timeout_s': 10},

    'e_fetcher_name': 'PlaywrightFetcher',
    'e_fetcher_init_param': {'log_callback': print, 'proxy': None, 'timeout_s': 20, 'stealth': True, 'pause_browser': False, 'render_page': True},

    'discoverer_name': 'RSSDiscoverer',
    'discoverer_init_param': {'verbose': True},

    'extractor_name': 'Trafilatura',
    'extractor_init_param': {'verbose': True},

    # ======== Crawl parameters ========
    
    'entry_points': ['https://www.ntv.com.tr/dunya.rss', 'https://www.ntv.com.tr/turkiye.rss', 'https://www.ntv.com.tr/ekonomi.rss', 'https://www.ntv.com.tr/gundem.rss'],
    'period_filter': (None, None),
    'channel_filter': None,
    'd_fetcher_kwargs': {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 10, 'scroll_pages': 0},
    'e_fetcher_kwargs': {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 20, 'scroll_pages': 0},
    'extractor_kwargs': {},

    # === Handler functions (to be implemented by user) ===
    
    'article_filter': None,
    'content_handler': None,
    'exception_handler': None,
}


# ------ Usage example and demo. Can be deleted. ------

if __name__ == "__main__":
    import traceback
    try:
        from IntelligenceCrawler.CrawlPipeline import run_pipeline, save_article_to_disk
        CRAWLER_CONFIG['content_handler'] = save_article_to_disk
        run_pipeline(CRAWLER_CONFIG)
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())
