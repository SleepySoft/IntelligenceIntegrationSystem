# CrawlerGenerated.py - This code is generated by CrawlerPlayground
from IntelligenceCrawler.CrawlPipeline import *

# === Fetcher init parameters ===
d_fetcher_init_param = {'log_callback': log_cb, 'proxy': 'http://127.0.0.1:10809', 'timeout_s': 30}
e_fetcher_init_param = {'log_callback': log_cb, 'proxy': 'http://127.0.0.1:10809', 'timeout_s': 30, 'stealth': True, 'pause_browser': False, 'render_page': True}

# === Crawl parameters ===
entry_point = ['https://www.nhk.or.jp/rss/news/cat1.xml', 'https://www.nhk.or.jp/rss/news/cat4.xml', 'https://www.nhk.or.jp/rss/news/cat5.xml', 'https://www.nhk.or.jp/rss/news/cat6.xml']
start_date = None
end_date = None
d_fetcher_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 20, 'scroll_pages': 0}
e_fetcher_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 40, 'scroll_pages': 0}
extractor_kwargs = {}
channel_filter_list = []

def run_pipeline(
        article_filter = lambda url, group: True,
        content_handler = save_article_to_disk,
        exception_handler = lambda url, exception: None,
        crawler_governor: Optional[GovernanceManager] = None
):
    # === 1. Initialize Components ===
    d_fetcher = RequestsFetcher(**d_fetcher_init_param)
    e_fetcher = PlaywrightFetcher(**e_fetcher_init_param)
    discoverer = RSSDiscoverer(fetcher=d_fetcher, verbose=True)
    extractor = TrafilaturaExtractor(verbose=True)

    # === 2. Build pipeline ===
    pipeline = CrawlPipeline(
        d_fetcher=d_fetcher,
        discoverer=discoverer,
        e_fetcher=e_fetcher,
        extractor=extractor,
        log_callback=log_cb,
        crawler_governor=crawler_governor
    )

    # Step 1: Discover all channels
    pipeline.discover_channels(entry_point, start_date, end_date,
                               fetcher_kwargs=d_fetcher_kwargs)

    # Step 2: Discover and fetch articles (populates pipeline.contents)
    pipeline.discover_articles(channel_filter=partial(
        common_channel_filter, channel_filter_list=channel_filter_list),
        fetcher_kwargs=d_fetcher_kwargs)

    # Step 3: Extract content and run handlers
    pipeline.extract_articles(
        article_filter=article_filter,
        content_handler=content_handler,
        exception_handler=exception_handler,
        fetcher_kwargs=e_fetcher_kwargs,
        extractor_kwargs=extractor_kwargs
    )


if __name__ == "__main__":
    try:
        run_pipeline()
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())


# -------------------------------------------- Manual Code Start --------------------------------------------

import random
from playwright.sync_api import Page
from CrawlerServiceEngine import ServiceContext
from MyPythonUtility.easy_config import EasyConfig
from Workflow.CommonFlowUtility import CrawlContext
from Workflow.CommonFeedsCrawFlow import build_crawl_ctx_by_service_ctx
from Workflow.IntelligenceCrawlFlow import (
    intelligence_crawler_result_handler,
    intelligence_crawler_fileter, \
    intelligence_crawler_exception_handler)

NAME = 'nhk'
config: EasyConfig | None = None
crawl_context: CrawlContext | None = None


def conditional_click_nhk(page: Page):
    # Main container for the popup - using class name
    modal_selector = "div._1o10k1w0"
    # Checkbox text
    checkbox_text = "内容について確認しました"
    # Button text
    button_text = "次へ"

    try:
        # 1. Wait for the modal to appear (timeout: 5 seconds)
        modal_locator = page.locator(modal_selector)
        modal_locator.wait_for(state="visible", timeout=5000)

        print(f"Popup detected ({modal_selector}), processing...")

        # 2. Locate and click the checkbox by text
        checkbox_label = page.get_by_text(checkbox_text, exact=True)
        checkbox_label.click()
        print("Checkbox clicked successfully.")

        # 3. Wait briefly for the button to become enabled
        page.wait_for_timeout(1000)

        # 4. Locate and click the button
        button_locator = page.get_by_text(button_text, exact=True)

        # Verify button is not disabled before clicking
        is_disabled = button_locator.get_attribute("disabled")
        if is_disabled is None:
            button_locator.click()
            print(f"Button '{button_text}' clicked successfully.")
        else:
            print("Button is still disabled, cannot proceed.")
            return False

        # 5. Wait for the modal to disappear
        try:
            modal_locator.wait_for(state="hidden", timeout=10000)
            print("Popup closed successfully.")
        except Exception:
            print("Warning: Popup did not close within the expected time.")

        return True

    except Exception as e:
        print("No popup detected, proceeding with scraping.")
        return False


def module_init(service_context: ServiceContext):
    global config
    global crawl_context
    config = service_context.config
    crawl_context = build_crawl_ctx_by_service_ctx(NAME, service_context)

    e_fetcher_kwargs['post_extra_action'] = conditional_click_nhk


def start_task(stop_event):
    # Crawl main process
    run_pipeline(
        article_filter=partial(intelligence_crawler_fileter, context=crawl_context),
        content_handler=partial(intelligence_crawler_result_handler, context=crawl_context),
        exception_handler=partial(intelligence_crawler_exception_handler, context=crawl_context),
        crawler_governor=crawl_context.crawler_governor
    )
    # Check and submit cached data.
    crawl_context.submit_cached_data(10)
    # Randomly delay for next crawl.
    # CrawlContext.wait_interruptibly(random.randint(10, 15) * 60, stop_event)
    crawl_context.crawler_governor.wait_interval(60 * 15, stop_event=stop_event)

# --------------------------------------------- Manual Code End ---------------------------------------------
