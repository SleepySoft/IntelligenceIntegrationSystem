# CrawlerGenerated.py - This code is generated by CrawlerPlayground
from IntelligenceCrawler.CrawlPipeline import *

# === Fetcher init parameters ===
d_fetcher_init_param = {'log_callback': log_cb, 'proxy': 'http://127.0.0.1:10809', 'timeout_s': 30}
e_fetcher_init_param = {'log_callback': log_cb, 'proxy': 'http://127.0.0.1:10809', 'timeout_s': 30, 'stealth': True, 'pause_browser': False, 'render_page': True}

# === Crawl parameters ===
entry_point = ['https://www.nhk.or.jp/rss/news/cat1.xml', 'https://www.nhk.or.jp/rss/news/cat4.xml', 'https://www.nhk.or.jp/rss/news/cat5.xml', 'https://www.nhk.or.jp/rss/news/cat6.xml']
start_date = None
end_date = None
d_fetcher_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 20, 'scroll_pages': 0}
e_fetcher_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 40, 'scroll_pages': 0}
extractor_kwargs = {}
channel_filter_list = []

def run_pipeline(
        article_filter = lambda url, group: True,
        content_handler = save_article_to_disk,
        exception_handler = lambda url, exception: None,
        crawler_governor: Optional[GovernanceManager] = None
):
    # === 1. Initialize Components ===
    d_fetcher = RequestsFetcher(**d_fetcher_init_param)
    e_fetcher = PlaywrightFetcher(**e_fetcher_init_param)
    discoverer = RSSDiscoverer(fetcher=d_fetcher, verbose=True)
    extractor = TrafilaturaExtractor(verbose=True)

    # === 2. Build pipeline ===
    pipeline = CrawlPipeline(
        d_fetcher=d_fetcher,
        discoverer=discoverer,
        e_fetcher=e_fetcher,
        extractor=extractor,
        log_callback=log_cb,
        crawler_governor=crawler_governor
    )

    # Step 1: Discover all channels
    pipeline.discover_channels(entry_point, start_date, end_date,
                               fetcher_kwargs=d_fetcher_kwargs)

    # Step 2: Discover and fetch articles (populates pipeline.contents)
    pipeline.discover_articles(channel_filter=partial(
        common_channel_filter, channel_filter_list=channel_filter_list),
        fetcher_kwargs=d_fetcher_kwargs)

    # Step 3: Extract content and run handlers
    pipeline.extract_articles(
        article_filter=article_filter,
        content_handler=content_handler,
        exception_handler=exception_handler,
        fetcher_kwargs=e_fetcher_kwargs,
        extractor_kwargs=extractor_kwargs
    )


if __name__ == "__main__":
    try:
        run_pipeline()
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())


# -------------------------------------------- Manual Code Start --------------------------------------------

import random
from playwright.sync_api import Page
from CrawlerServiceEngine import ServiceContext
from MyPythonUtility.easy_config import EasyConfig
from Workflow.CommonFlowUtility import CrawlContext
from Workflow.CommonFeedsCrawFlow import build_crawl_ctx_by_service_ctx
from Workflow.IntelligenceCrawlFlow import (
    intelligence_crawler_result_handler,
    intelligence_crawler_fileter, \
    intelligence_crawler_exception_handler)

NAME = 'nhk'
config: EasyConfig | None = None
crawl_context: CrawlContext | None = None


# 同时兼容两种确认方式（或许是受网络影响，网页弹出的确认界面可能不一样）。

def conditional_click_nhk(page: Page):
    """
    Enhanced function to handle multiple popup variants on NHK website.
    Returns True if popup was found and processed, False otherwise.
    """
    # Variant 1: Checkbox + Button pattern (new style)
    checkbox_text = "内容について確認しました"
    next_button_text = "次へ"

    # Variant 2: Direct button pattern (old style)
    direct_button_text = "確認しました / I understand"

    try:
        # First, try to detect and handle Variant 1 (checkbox + button pattern)
        try:
            # Wait for checkbox to appear (shorter timeout for detection)
            checkbox_locator = page.get_by_text(checkbox_text, exact=True)
            if checkbox_locator.is_visible(timeout=3000):
                print("Detected Variant 1 popup (checkbox + button), processing...")

                # Click the checkbox
                checkbox_locator.click()
                print("Checkbox clicked successfully.")

                # Wait for button to become enabled
                page.wait_for_timeout(1000)

                # Locate and click the next button
                next_button = page.get_by_text(next_button_text, exact=True)

                # Check if button is enabled
                is_disabled = next_button.get_attribute("disabled")
                if is_disabled is None:
                    next_button.click()
                    print(f"Button '{next_button_text}' clicked successfully.")

                    # Wait for popup to disappear
                    page.wait_for_timeout(2000)
                    print("Variant 1 popup handled successfully.")
                    return True
                else:
                    print("Next button is still disabled, trying alternative approach...")
                    return False

        except Exception as e:
            print(f"Variant 1 not detected or failed: {str(e)}")

        # If Variant 1 not found, try Variant 2 (direct button pattern)
        try:
            direct_button = page.get_by_text(direct_button_text, exact=True)
            if direct_button.is_visible(timeout=3000):
                print("Detected Variant 2 popup (direct button), processing...")

                # Click the direct button
                direct_button.click()
                print(f"Button '{direct_button_text}' clicked successfully.")

                # Wait for popup to disappear
                page.wait_for_timeout(2000)
                print("Variant 2 popup handled successfully.")
                return True

        except Exception as e:
            print(f"Variant 2 not detected or failed: {str(e)}")

        # If neither variant is detected
        print("No known popup variant detected, continuing with scraping.")
        return False

    except Exception as e:
        print(f"Unexpected error during popup handling: {str(e)}")
        return False


def module_init(service_context: ServiceContext):
    global config
    global crawl_context
    config = service_context.config
    crawl_context = build_crawl_ctx_by_service_ctx(NAME, service_context)

    e_fetcher_kwargs['post_extra_action'] = conditional_click_nhk


def start_task(stop_event):
    # Crawl main process
    run_pipeline(
        article_filter=partial(intelligence_crawler_fileter, context=crawl_context),
        content_handler=partial(intelligence_crawler_result_handler, context=crawl_context),
        exception_handler=partial(intelligence_crawler_exception_handler, context=crawl_context),
        crawler_governor=crawl_context.crawler_governor
    )
    # Check and submit cached data.
    crawl_context.submit_cached_data(10)
    # Randomly delay for next crawl.
    # CrawlContext.wait_interruptibly(random.randint(10, 15) * 60, stop_event)
    crawl_context.crawler_governor.wait_interval(60 * 15, stop_event=stop_event)

# --------------------------------------------- Manual Code End ---------------------------------------------
