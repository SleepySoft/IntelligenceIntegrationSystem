# CrawlerGenerated.py - This code is generated by CrawlerPlayground

from IntelligenceCrawler.CrawlPipeline import *


def run_pipeline(
        article_filter = lambda url: True,
        content_handler = save_article_to_disk,
        exception_handler = lambda url, exception: None
):
    # === 1. Initialize Components ===
    d_fetcher = RequestsFetcher(log_callback=log_cb, proxy='http://127.0.0.1:10809', timeout_s=10)
    e_fetcher = PlaywrightFetcher(log_callback=log_cb, proxy='http://127.0.0.1:10809', timeout_s=20, stealth=False, pause_browser=False, render_page=True)
    discoverer = RSSDiscoverer(fetcher=d_fetcher, verbose=True)
    extractor = TrafilaturaExtractor(verbose=True)

    # === 2. Define Parameters ===
    entry_point = ['https://www.nhk.or.jp/rss/news/cat1.xml', 'https://www.nhk.or.jp/rss/news/cat4.xml', 'https://www.nhk.or.jp/rss/news/cat5.xml', 'https://www.nhk.or.jp/rss/news/cat6.xml']
    start_date = None
    end_date = None
    d_fetcher_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 10, 'scroll_pages': 0}
    e_fetcher_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 20, 'scroll_pages': 0}
    extractor_kwargs = {}
    channel_filter_list = []

    # === 3. Build pipeline ===
    pipeline = CrawlPipeline(
        d_fetcher=d_fetcher,
        discoverer=discoverer,
        e_fetcher=e_fetcher,
        extractor=extractor,
        log_callback=log_cb
    )

    # Step 1: Discover all channels
    pipeline.discover_channels(entry_point, start_date, end_date,
                               fetcher_kwargs=d_fetcher_kwargs)

    # Step 2: Discover and fetch articles (populates pipeline.contents)
    pipeline.discover_articles(channel_filter=partial(
        common_channel_filter, channel_filter_list=channel_filter_list),
        fetcher_kwargs=d_fetcher_kwargs)

    # Step 3: Extract content and run handlers
    pipeline.extract_articles(
        article_filter=article_filter,
        content_handler=content_handler,
        exception_handler=exception_handler,
        fetcher_kwargs=e_fetcher_kwargs,
        extractor_kwargs=extractor_kwargs
    )


if __name__ == "__main__":
    try:
        run_pipeline()
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())


# -------------------------------------------- Manual Code Start --------------------------------------------

import random
from ServiceEngine import ServiceContext
from MyPythonUtility.easy_config import EasyConfig
from Workflow.CommonFlowUtility import CrawlContext
from Workflow.CommonFeedsCrawFlow import build_crawl_ctx_by_config
from Workflow.IntelligenceCrawlFlow import (
    intelligence_crawler_result_handler,
    intelligence_crawler_fileter, \
    intelligence_crawler_exception_handler)

NAME = 'nhk'
config: EasyConfig | None = None
crawl_context: CrawlContext | None = None


def module_init(service_context: ServiceContext):
    global config
    global crawl_context
    config = service_context.config
    crawl_context = build_crawl_ctx_by_config(NAME, config)


def start_task(stop_event):
    # Crawl main process
    run_pipeline(
        article_filter=partial(intelligence_crawler_fileter, context=crawl_context, levels=[NAME]),
        content_handler=partial(intelligence_crawler_result_handler, context=crawl_context, levels=[NAME]),
        exception_handler=partial(intelligence_crawler_exception_handler, context=crawl_context, levels=[NAME])
    )
    # Check and submit cached data.
    crawl_context.submit_cached_data(10)
    # Randomly delay for next crawl.
    CrawlContext.wait_interruptibly(random.randint(10, 15) * 60, stop_event)

# --------------------------------------------- Manual Code End ---------------------------------------------
